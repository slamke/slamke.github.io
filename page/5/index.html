
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>雁渡寒潭 风吹疏竹</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Sun Ke">
    

    
    <meta name="description" content="人生不止眼前的苟且">
<meta property="og:type" content="website">
<meta property="og:title" content="雁渡寒潭 风吹疏竹">
<meta property="og:url" content="slamke.github.io/page/5/index.html">
<meta property="og:site_name" content="雁渡寒潭 风吹疏竹">
<meta property="og:description" content="人生不止眼前的苟且">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="雁渡寒潭 风吹疏竹">
<meta name="twitter:description" content="人生不止眼前的苟且">

    
    <link rel="alternative" href="/atom.xml" title="雁渡寒潭 风吹疏竹" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="雁渡寒潭 风吹疏竹" title="雁渡寒潭 风吹疏竹"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="雁渡寒潭 风吹疏竹">雁渡寒潭 风吹疏竹</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:slamke.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/02/23/Spark有用的配置选项/" title="Spark有用的配置选项" itemprop="url">Spark有用的配置选项</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-02-23T08:58:04.000Z" itemprop="datePublished"> Published 2017-02-23</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="任务提交"><a href="#任务提交" class="headerlink" title="任务提交"></a>任务提交</h2><ul>
<li><figure class="highlight plain"><figcaption><span>spark任务对应的Jar，配置该选项后，每次提交任务不用上传该assembly.jar,减少了任务启动时间</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">``` shell</span><br><span class="line">spark.yarn.jar hdfs://****:****/path/spark/spark-assembly-***.jar</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight plain"><figcaption><span>yarn提交任务时的黑名单</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">## Shuffle</span><br><span class="line"></span><br><span class="line">- ```spark.shuffle.file.buffer``` </span><br><span class="line"></span><br><span class="line">Size of the in-memory buffer for each shuffle file output stream. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files.</span><br><span class="line"></span><br><span class="line">## Memory</span><br><span class="line"></span><br><span class="line">- ```spark.yarn.executor.memoryOverhead``` </span><br><span class="line">The amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Spark SQL</span><br><span class="line"></span><br><span class="line">- ``` spark.sql.shuffle.partitions</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>Spark SQL 2.0版本之前，reduce的个数是通过spark.default.parallelism和spark.sql.shuffle.partitions两个参数进行配置。如果配置过大，将会导致下游产生很多碎片化的Task，或者下游HDFS产生很多小文件。如果设置过小，将会导致单个ReduceTask计算负载过大。</p>
<ul>
<li><figure class="highlight plain"><figcaption><span>```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL 2.0+支持通过spark.sql.adaptive.enabled来设置reduce大小自适应</span><br><span class="line">- ```spark.sql.files.ignoreCorruptFiles</span><br></pre></td></tr></table></figure></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/01/12/Spark2-0-DataSource-API/" title="Spark2.0 DataSource API" itemprop="url">Spark2.0 DataSource API</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-01-12T07:51:50.000Z" itemprop="datePublished"> Published 2017-01-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="Spark-1-如何实现Spark-External-Datasource"><a href="#Spark-1-如何实现Spark-External-Datasource" class="headerlink" title="Spark 1 如何实现Spark External Datasource"></a>Spark 1 如何实现Spark External Datasource</h2><p><a href="https://www.mapr.com/blog/spark-data-source-api-extending-our-spark-sql-query-engine" target="_blank" rel="external">Spark Data Source API: Extending Our Spark SQL Query Engine</a></p>
<p><a href="http://blog.csdn.net/oopsoom/article/details/42061077" target="_blank" rel="external">Spark SQL之External DataSource外部数据源（一）示例</a></p>
<p><a href="http://blog.csdn.net/oopsoom/article/details/42064075" target="_blank" rel="external">Spark SQL之External DataSource外部数据源（二）源码分析</a></p>
<p>参考实现：</p>
<ul>
<li><a href="https://github.com/SequoiaDB/spark-sequoiadb" target="_blank" rel="external">spark-sequoiadb</a></li>
<li><a href="https://github.com/databricks/spark-csv" target="_blank" rel="external">spark-csv</a></li>
<li><a href="https://github.com/RedisLabs/spark-redis" target="_blank" rel="external">spark-redis</a></li>
<li><a href="http://geek.csdn.net/news/detail/76853" target="_blank" rel="external">Spark多数据源计算实践及其在GrowingIO的实践</a></li>
</ul>
<p><a href="http://wiki.baidu.com/pages/viewpage.action?pageId=213816907" target="_blank" rel="external">no</a></p>
<h2 id="Spark-2-如何实现Spark-External-Datasource"><a href="#Spark-2-如何实现Spark-External-Datasource" class="headerlink" title="Spark 2 如何实现Spark External Datasource"></a>Spark 2 如何实现Spark External Datasource</h2><p><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource-api.html" target="_blank" rel="external">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource-api.html</a></p>
<p><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource.html" target="_blank" rel="external">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource.html</a></p>
<p><a href="http://www.spark.tc/exploring-the-apache-spark-datasource-api/" target="_blank" rel="external">http://www.spark.tc/exploring-the-apache-spark-datasource-api/</a></p>
<p>新特性：</p>
<ul>
<li>子查询的自持</li>
<li>更加丰富的读写api支持，包括<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">- RelationProvider</span><br><span class="line"></span><br><span class="line">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource-api.html</span><br><span class="line"></span><br><span class="line">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource.html</span><br><span class="line"></span><br><span class="line">http://www.spark.tc/exploring-the-apache-spark-datasource-api/</span><br><span class="line"></span><br><span class="line">[关键](http://wiki.baidu.com/pages/viewpage.action?pageId=213816907)</span><br><span class="line"></span><br><span class="line">- DataSourceRegister</span><br><span class="line"></span><br><span class="line">triat ```DataSourceRegister``` is an interface to register DataSources under their shortName aliases (to look them up later).</span><br><span class="line">``` scala</span><br><span class="line">package org.apache.spark.sql.sources</span><br><span class="line"></span><br><span class="line">trait DataSourceRegister &#123;</span><br><span class="line">  def shortName(): String</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>It allows users to use the data source alias as the format type over the fully qualified class name.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Spark/">Spark</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Spark2-0/">Spark2.0</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/01/12/Hive-MapReduce-Spark分布式生成唯一数值型ID/" title="Hive MapReduce Spark分布式生成唯一数值型ID" itemprop="url">Hive MapReduce Spark分布式生成唯一数值型ID</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-01-12T06:25:35.000Z" itemprop="datePublished"> Published 2017-01-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>转载自： <a href="http://lxw1234.com/archives/2016/12/798.htm" target="_blank" rel="external">http://lxw1234.com/archives/2016/12/798.htm</a></p>
<p>在实际业务场景下，经常会遇到在Hive、MapReduce、Spark中需要生成唯一的数值型ID。</p>
<p>一般常用的做法有：</p>
<ol>
<li>MapReduce中使用1个Reduce来生成；</li>
<li>Hive中使用row_number分析函数来生成，其实也是1个Reduce；</li>
<li>借助HBase或Redis或Zookeeper等其它框架的计数器来生成；</li>
</ol>
<p>数据量不大的情况下，可以直接使用1和2方法来生成，但如果数据量巨大，1个Reduce处理起来就非常慢。</p>
<p>在数据量非常大的情况下，如果你仅仅需要唯一的数值型ID，注意：不是需要”连续的唯一的数值型ID”，那么可以考虑采用本文中介绍的方法，否则，请使用第3种方法来完成。</p>
<p>Spark中生成这样的非连续唯一数值型ID，非常简单，直接使用zipWithUniqueId()即可。</p>
<p>关于zipWithUniqueId，可参考：<a href="http://lxw1234.com/archives/2015/07/352.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/07/352.htm</a></p>
<p>参考zipWithUniqueId()的方法，在MapReduce和Hive中，实现如下：<br><img src="http://7xipth.com1.z0.glb.clouddn.com/20161206-5.jpg" alt=""></p>
<p>在Spark中，zipWithUniqueId是通过使用分区Index作为每个分区ID的开始值，在每个分区内，ID增长的步长为该RDD的分区数，那么在MapReduce和Hive中，也可以照此思路实现，Spark中的分区数，即为MapReduce中的Map数，Spark分区的Index，即为Map Task的ID。Map数，可以通过JobConf的getNumMapTasks()，而Map Task ID，可以通过参数mapred.task.id获取，格式如：attempt_1478926768563_0537_m_000004_0，截取m_000004_0中的4，再加1，作为该Map Task的ID起始值。注意：这两个只均需要在Job运行时才能获取。另外，从图中也可以看出，每个分区/Map Task中的数据量不是绝对一致的，因此，生成的ID不是连续的。</p>
<p>下面的UDF可以在Hive中直接使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lxw1234.hive.udf;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.MapredContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.UDFType;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"> </span><br><span class="line"><span class="meta">@UDFType</span>(deterministic = <span class="keyword">false</span>, stateful = <span class="keyword">true</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RowSeq2</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> LongWritable result = <span class="keyword">new</span> LongWritable();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">char</span> SEPARATOR = <span class="string">'_'</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ATTEMPT = <span class="string">"attempt"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> initID = <span class="number">0l</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> increment = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(MapredContext context)</span> </span>&#123;</span><br><span class="line">        increment = context.getJobConf().getNumMapTasks();</span><br><span class="line">        <span class="keyword">if</span>(increment == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"mapred.map.tasks is zero"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        initID = getInitId(context.getJobConf().get(<span class="string">"mapred.task.id"</span>),increment);</span><br><span class="line">        <span class="keyword">if</span>(initID == <span class="number">0l</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"mapred.task.id"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        System.out.println(<span class="string">"initID : "</span> + initID + <span class="string">"  increment : "</span> + increment);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span></span><br><span class="line">            <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.writableLongObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        result.set(getValue());</span><br><span class="line">        increment(increment);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] children)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"RowSeq-func()"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">increment</span><span class="params">(<span class="keyword">int</span> incr)</span> </span>&#123;</span><br><span class="line">        initID += incr;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">getValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> initID;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//attempt_1478926768563_0537_m_000004_0 // return 0+1</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">getInitId</span> <span class="params">(String taskAttemptIDstr,<span class="keyword">int</span> numTasks)</span></span><br><span class="line">            <span class="keyword">throws</span> IllegalArgumentException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            String[] parts = taskAttemptIDstr.split(Character.toString(SEPARATOR));</span><br><span class="line">            <span class="keyword">if</span>(parts.length == <span class="number">6</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span>(parts[<span class="number">0</span>].equals(ATTEMPT)) &#123;</span><br><span class="line">                    <span class="keyword">if</span>(!parts[<span class="number">3</span>].equals(<span class="string">"m"</span>) &amp;&amp; !parts[<span class="number">3</span>].equals(<span class="string">"r"</span>)) &#123;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> Exception();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">long</span> result = Long.parseLong(parts[<span class="number">4</span>]);</span><br><span class="line">                    <span class="keyword">if</span>(result &gt;= numTasks) &#123; <span class="comment">//if taskid &gt;= numtasks</span></span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"TaskAttemptId string : "</span> + taskAttemptIDstr</span><br><span class="line">                                + <span class="string">"  parse ID ["</span> + result + <span class="string">"] &gt;= numTasks["</span> + numTasks + <span class="string">"] .."</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">return</span> result + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;&#125;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"TaskAttemptId string : "</span> + taskAttemptIDstr</span><br><span class="line">                + <span class="string">" is not properly formed"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Hive/">Hive</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Hive/">Hive</a><a href="/tags/UDF/">UDF</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/01/12/Spark2-0学习笔记/" title="Spark2.0学习笔记" itemprop="url">Spark2.0学习笔记</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-01-12T03:18:42.000Z" itemprop="datePublished"> Published 2017-01-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="Catalog-API"><a href="#Catalog-API" class="headerlink" title="Catalog API"></a>Catalog API</h2><p>Spark中的DataSet和Dataframe API支持结构化分析。结构化分析的一个重要的方面是管<br>理元数据。这些元数据可能是一些临时元数据（比如临时表）、SQLContext上注册的UDF以及<br>持久化的元数据（比如Hivemeta store或者HCatalog）。<br>Spark的早期版本是没有标准的API来访问这些元数据的。用户通常使用查询语句（比<br>如 show tables ）来查询这些元数据。这些查询通常需要操作原始的字符串，而且不同元数据<br>类型的操作也是不一样的。<br>这种情况在Spark 2.0中得到改变。Spark 2.0中添加了标准的API（称为catalog）来访问<br>Spark SQL中的元数据。这个API既可以操作Spark SQL，也可以操作Hive元数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder.appName(<span class="string">"spark session example"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"><span class="keyword">val</span> catalog = sparkSession.catalog</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Querying the databases<br>一旦创建好catalog对象之后，可以使用它来查询元数据中的数据库，catalog上的<br>API返回的结果全部都是dataset。<br>listDatabases 返回元数据中所有的数据库。默认情况下，元数据仅仅只有名为default的数<br>据库。如果是Hive元数据，那么它会从Hive元数据中获取所有的数据库。 listDatabases 返<br>回的类型是dataset，所以我们可以使用Dataset上的所有操作来查询元数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catalog.listDatabases().select(<span class="string">"name"</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用createTempView注册Dataframe<br>在Spark的早期版本，我们使用 registerTempTable 来注册Dataframe。然而在Spark<br>2.0中，这个API已经被遗弃了。 registerTempTable 名字很让人误解，因为用户会认为这个函<br>数会将Dataframe持久化并且保证这个临时表，但是实际上并不是这样的，所以社区才有意将它<br>替换成 createTempView 。 createTempView 的使用方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createTempView(<span class="string">"iteblog"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表<br>正如我们可以展示出元数据中的所有数据库一样，我们也可以展示出元数据中某个数据库中<br>的表。它会展示出Spark SQL中所有注册的临时表。同时可以展示出Hive中默认数据库（也就是<br>default）中的表。如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catalog.listTables().select(<span class="string">"name"</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断某个表是否缓存</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">catalog.isCached(<span class="string">"iteblog"</span>)</span><br><span class="line">df.cache()</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除view<br>以使用catalog提供的API来删除view。如果是Spark SQL情况，那么它会删除事先注册好的view；如果是hive情况，那么它会从元数据中删除表</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catalog.dropTempView(<span class="string">"iteblog"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询已经注册的函数<br>仅可以使用Catalog API操作表，还可以用它操作UDF。下面代码片段展示<br>SparkSession上所有已经注册号的函数，当然也包括了Spark内置的函数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catalog.listFunctions().select(<span class="string">"name"</span>,<span class="string">"className"</span>,<span class="string">"isTemporary"</span>).show(<span class="number">100</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Catalyst优化器"><a href="#Catalyst优化器" class="headerlink" title="Catalyst优化器"></a>Catalyst优化器</h2><p>Spark SQL使用Catalyst优化所有的查询，包括spark sql和dataframe dsl。这个优化器的使用使得查询比直接使用RDD要快很多。Spark在每个版本都会对Catalyst进行优化以便提高查询性能，而不需要用户修改他们的代码。<br>Catalyst是一个单独的模块类库，这个模块是基于规则的系统。这个框架中的每个规则都是针对某个特定的情况来优化的。比如： ConstantFolding 规则用于移除查询中的常量表达式。<br>在Spark的早期版本，如果需要添加自定义的优化规则，我们需要修改Spark的源码，这在很多情况下是不太可取的，比如我们仅仅需要优化特定的领域或者场景。所以开发社区想有一种可插拨的方式在Catalyst中添加优化规则。值得高兴的是，Spark 2.0提供了这种实验式的API，我们可以基于这些API添加自定义的优化规则。本文将介绍如何编写自定义的优化规则，并将这些规则添加到Catalyst中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> 	<span class="comment">// dataframe的优化计划(Optimized plan)</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.option(<span class="string">"header"</span>, <span class="string">"true"</span>).csv(<span class="string">"file:///user/iteblog/sales.csv"</span>)</span><br><span class="line">    <span class="keyword">val</span> multipliedDF = df.selectExpr(<span class="string">"amountPaid * 1"</span>)</span><br><span class="line">    println(multipliedDF.queryExecution.optimizedPlan.numberedTreeString)</span><br><span class="line">	<span class="comment">// Spark自动对每一行的 amountPaid 乘上 1.0 。但是这不是最优计划！因为如果是乘以1，最终的结果是一样的。所有我们可以利用这个知识来编写自定义的优化规则，并将这个规则加入到Catalyst中</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义优化计划</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.catalyst.expressions.&#123;<span class="type">Literal</span>, <span class="type">Multiply</span>&#125;</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.catalyst.plans.logical.<span class="type">LogicalPlan</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.catalyst.rules.<span class="type">Rule</span></span><br><span class="line">    <span class="class"><span class="keyword">object</span> <span class="title">MultiplyOptimizationRule</span> <span class="keyword">extends</span> <span class="title">Rule</span>[<span class="type">LogicalPlan</span>] </span>&#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">LogicalPlan</span> = plan transformAllExpressions &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Multiply</span>(left, right) <span class="keyword">if</span> right.isInstanceOf[<span class="type">Literal</span>] &amp;&amp;</span><br><span class="line">          right.asInstanceOf[<span class="type">Literal</span>].value.asInstanceOf[<span class="type">Double</span>] == <span class="number">1.0</span> =&gt;</span><br><span class="line">          println(<span class="string">"optimization of one applied"</span>)</span><br><span class="line">          left</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// 我们扩展了Rule，Rule是直接操作逻辑计划的。绝大多数的规则都是使用Scala中的模式匹</span></span><br><span class="line">配。在上面的代码中，我们首先判断优化的操作数(operand)是否是文字(literal)，然后判断其值</span><br><span class="line">是否是<span class="number">1.0</span>。为了简便起见，我们限定了<span class="number">1</span>出现的位置，如果<span class="number">1</span>出现在左边，这个优化规则将不起</span><br><span class="line">作用。但是我们可以仿照上面的示例轻松地实现.通过上面的规则，如果右边的值是<span class="number">1</span>，我们将直接返回左边的值。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 将自定义的优化规则加入到Catalyst中</span></span><br><span class="line">    spark.experimental.extraOptimizations = <span class="type">Seq</span>(<span class="type">MultiplyOptimizationRule</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用自定义优化规则</span></span><br><span class="line">    <span class="keyword">val</span> newMultipliedDF = df.selectExpr(<span class="string">"amountPaid * 1"</span>)</span><br><span class="line">    println(newMultipliedDF.queryExecution.optimizedPlan.numberedTreeString)</span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL中Window-API"><a href="#Spark-SQL中Window-API" class="headerlink" title="Spark SQL中Window API"></a>Spark SQL中Window API</h2><p>Spark SQL中的window API是从1.4版本开始引入的，以便支持更智能的分组功能。这个功<br>能对于那些有SQL背景的人来说非常有用；但是在Spark 1.x中，window API一大缺点就是无法<br>使用时间来创建窗口。时间在诸如金融、电信等领域有着非常重要的角色，基于时间来理解数据<br>变得至关重要。<br>不过值得高兴的是，在Spark 2.0中，window API内置也支持time windows！Spark SQL<br>中的time windows和Spark Streaming中的time windows非常类似。</p>
<ul>
<li><p>tumbling window<br>所有的timewindow API需要一个类型为timestamp的列。<figure class="highlight plain"><figcaption><span>by```语句中使用。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">window方法的第一个参数指定了时间所在的列；第二个参数指定了窗口的持续时间</span><br><span class="line">(duration)，它的单位可以是seconds、minutes、hours、days或者weeks。创建好窗口之</span><br><span class="line">后，我们可以计算平均值。</span><br><span class="line">``` scala</span><br><span class="line">// 使用了内置的year函数来提取出日期中的年</span><br><span class="line">val stocks2016 = stocksDF.filter(&quot;year(Date)==2016&quot;)</span><br><span class="line"></span><br><span class="line">// 对每个星期创建一个窗口，这种类型的窗口通常被称为tumbling window</span><br><span class="line">val tumblingWindowDS = stocks2016.groupBy(window(stocks2016.col(&quot;Date&quot;),&quot;1 week&quot;)).agg(avg(&quot;Close&quot;).as(&quot;weekly_average&quot;))</span><br><span class="line"></span><br><span class="line">def printWindow(windowDF:DataFrame, aggCol:String) =&#123;</span><br><span class="line">windowDF.sort(&quot;window.start&quot;).</span><br><span class="line">select(&quot;window.start&quot;,&quot;window.end&quot;,s&quot;$aggCol&quot;).</span><br><span class="line">show(truncate = false)</span><br><span class="line">&#125;</span><br><span class="line">// 打印window的值</span><br><span class="line">printWindow(tumblingWindowDS,&quot;weekly_average&quot;)</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>用sliding window（滑动窗口）<br>到目前为止，没有相关API来创建带有开始时间的tumbling<br>window，但是我们可以通过将窗口时间(window duration)和滑动时间(slide duration)设置成<br>一样来创建带有开始时间的tumbling window。</p>
</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 4 days 参数就是开始时间的偏移量；前两个参数分别代表窗口时间和滑动时间</span><br><span class="line">val iteblogWindowWithStartTime = stocks2016.groupBy(window(stocks2016.col("Date"),"1</span><br><span class="line">week","1 week", "4 days")).agg(avg("Close").as("weekly_average"))</span><br></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Spark/">Spark</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Spark2-0/">Spark2.0</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/01/05/HFTP文件系统解析/" title="HFTP文件系统解析" itemprop="url">HFTP文件系统解析</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-01-05T07:57:58.000Z" itemprop="datePublished"> Published 2017-01-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="初探"><a href="#初探" class="headerlink" title="初探"></a>初探</h2><p>抽象类<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">|  文件系统  |  URI前缀  |  hadoop的具体实现类 |</span><br><span class="line">| --- | --- | --- |</span><br><span class="line">| Local | file | fs.LocalFileSystem |</span><br><span class="line">| HDFS  | hdfs | hdfs.DistributedFileSystem |</span><br><span class="line">| HFTP  | hftp | hdfs.HftpFileSystem |</span><br><span class="line">| HSFTP | hsftp | hdfs.HsftpFileSystem |</span><br><span class="line">| HAR  | har  | fs.HarFileSystem |</span><br><span class="line">| KFS  | kfs  | fs.kfs.KosmosFileSystem |</span><br><span class="line">| FTP  | ftp  | fs.ftp.FTPFileSystem |</span><br><span class="line">| S3 (native) | s3n | s.s3native.NativeS3FileSystem |</span><br><span class="line">| S3 (blockbased) | s3 | fs.s3.S3FileSystem |</span><br><span class="line"></span><br><span class="line">Hadoop提供了很多接口来访问这些文件系统，最常用的是通过URI前缀来访问正确的文件系统。比如:</span><br><span class="line">&gt; hadoop fs -ls  file:///.......</span><br><span class="line"> &gt; hadoop fs -ls  hdfs:///.......</span><br><span class="line"></span><br><span class="line">虽然理论上MapReduce可以使用上面这些系统，但是如果我们处理海量数据的话还是要选用一个分布式文件系统hdfs或者kfs。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 配置</span><br><span class="line">hadoop-default.xml关于filesystem实现的配置```hadoop-default.xml```：</span><br><span class="line">``` xml</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.file.impl&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.fs.LocalFileSystem&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The FileSystem for file: uris.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.fms.impl&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hdfs.FMSFileSystem&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The FileSystem for hdfs: uris.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.hdfs.impl&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hdfs.DistributedFileSystem&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The FileSystem for hdfs: uris.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<p>spark-client关于filesystem实现的配置<figure class="highlight plain"><figcaption><span>```：</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">``` xml</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.file.impl&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.fs.LocalFileSystem&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;The FileSystem for file: uris.&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  </span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.hdfs.impl&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.fs.DFileSystem&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">   &lt;name&gt;fs.webhdfs.impl&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;org.apache.hadoop.smw.hdfs.web.WebHdfsFileSystem&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br></pre></td></tr></table></figure></p>
<h2 id="HFTP介绍"><a href="#HFTP介绍" class="headerlink" title="HFTP介绍"></a>HFTP介绍</h2><p>HFTP 是hadoop文件系统用来让你从一个远程的hadoop HDFS集群中读取数据的组件。这个读取是通过HTTP，并且数据源是DataNodes。HFTP是一个只读的文件系统，当你试图用来写入数据或者修改文件系统状态时，会抛出异常。</p>
<p>HFTP 主要的帮助在有多个HDFS集群，并存在多个版本时，将数据从一个集群迁移到另一个。HFTP 在不同版本的HDFS中是兼容写的。你可以操作例如：</p>
<blockquote>
<p>hadoop distcp -i hftp://sourceFS:50070/src hdfs://destFS:8020/dest</p>
</blockquote>
<p>注意HFTP是只读的，所以目标端必须是HDFS文件系统。（在这个例子中，distcp会使用新文件系统的配置运行。）</p>
<p>另外，HSFTP，默认使用HTTPS。这意味着数据在传输的时候会被加密。<br><a href="https://hadoop.apache.org/docs/r2.6.3/hadoop-project-dist/hadoop-hdfs/Hftp.html" target="_blank" rel="external">https://hadoop.apache.org/docs/r2.6.3/hadoop-project-dist/hadoop-hdfs/Hftp.html</a></p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>HFTP的代码在java 类<code>org.apache.hadoop.hdfs.HftpFileSystem</code> 中。同样的，HSFTP也在<code>org.apache.hadoop.hdfs.HsftpFileSystem</code>中实现.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/HDFS/">HDFS</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/HDFS/">HDFS</a><a href="/tags/HFTP/">HFTP</a><a href="/tags/Hadoop/">Hadoop</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/01/02/SparkListener机制详解/" title="SparkListener机制详解" itemprop="url">SparkListener机制详解</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-01-02T10:35:49.000Z" itemprop="datePublished"> Published 2017-01-02</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Spark源码注释中有下面一句话:</p>
<blockquote>
<p>Asynchronously passes SparkListenerEvents to registered SparkListeners</p>
</blockquote>
<p>即所有spark消息SparkListenerEvents 被异步的发送给已经注册过的SparkListeners.<br>在SparkContext中, 首先会创建LiveListenerBus实例,这个类主要功能如下:</p>
<ul>
<li>保存所有消息队列,负责消息的缓存</li>
<li>保存所有注册过的listener,负责消息的分发<br><img src="http://img.blog.csdn.net/20150806141900394" alt=""></li>
</ul>
<p>listener链表保存在ListenerBus类中,为了保证并发访问的安全性,此处采用Java的CopyOnWriteArrayList类来存储listener. 当需要对listener链表进行更改时,CopyOnWriteArrayList的特性使得会先复制整个链表,然后在复制的链表上面进行修改.当一旦获得链表的迭代器,在迭代器的生命周期中,可以保证数据的一致性.</p>
<p>消息队列实际上是保存在类AsynchronousListenerBus中的:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">EVENT_QUEUE_CAPACITY</span> = <span class="number">10000</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> eventQueue = <span class="keyword">new</span> <span class="type">LinkedBlockingQueue</span>[<span class="type">E</span>](<span class="type">EVENT_QUEUE_CAPACITY</span>)</span><br></pre></td></tr></table></figure>
<p>事件队列的长度为10000,当缓存事件数量达到上限后,新来的事件会被丢弃,</p>
<p>在SparkContext中,会</p>
<ul>
<li>创建LiveListenerBus类类型的成员变量listenerBus</li>
<li>创建各种listener,并加入到listenerBus中</li>
<li>post一些事件到listenerBus中</li>
<li>调用listenerBus.start() 来启动事件处理程序</li>
</ul>
<p><img src="http://img.blog.csdn.net/20150806141223019" alt=""></p>
<p>这里有一点需要注意的是, 在listenerBus.start() 调用之前, 可以向其中post消息, 这些消息会被缓存起来,等start函数调用之后, 消费者线程会分发这些缓存的消息.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/12/23/使用RCU技术实现读写线程无锁/" title="使用RCU技术实现读写线程无锁" itemprop="url">使用RCU技术实现读写线程无锁</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2016-12-23T06:56:16.000Z" itemprop="datePublished"> Published 2016-12-23</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>在一个系统中有一个写线程和若干个读线程，读写线程通过一个指针共用了一个数据结构，写线程改写这个结构，读线程读取该结构。在写线程改写这个数据结构的过程中，加锁情况下读线程由于等待锁耗时会增加。</p>
<p>可以利用RCU (Read Copy Update What is rcu)的思想来去除这个锁。</p>
<h2 id="RCU"><a href="#RCU" class="headerlink" title="RCU"></a>RCU</h2><p>RCU可以说是一种替代读写锁的方法。其基于一个事实：当写线程在改变一个指针时，读线程获取这个指针，要么获取到老的值，要么获取到新的值。RCU的基本思想其实很简单，参考<a href="http://www.rdrop.com/~paulmck/RCU/whatisRCU.html" target="_blank" rel="external">What is RCU</a>中Toy implementation可以很容易理解。一种简单的RCU流程可以描述为：</p>
<p>写线程：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">old_ptr = _ptr</span><br><span class="line">tmp_ptr = copy(_ptr)     <span class="comment">// copy</span></span><br><span class="line">change(tmp_ptr)          <span class="comment">// change </span></span><br><span class="line">_ptr = tmp_ptr           <span class="comment">// update</span></span><br><span class="line">synchroize(tmp_ptr)</span><br></pre></td></tr></table></figure></p>
<p>写线程要更新_ptr指向的内容时，先复制一份新的，基于新的进行改变，更新_ptr指针，最后同步释放老的内存。</p>
<p>读线程：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tmp_ptr = _<span class="function">ptr</span><br><span class="line"><span class="title">use</span><span class="params">(tmp_ptr)</span></span><br><span class="line"><span class="title">dereference</span><span class="params">(tmp_ptr)</span></span></span><br></pre></td></tr></table></figure></p>
<p>读线程直接使用_ptr，使用完后需要告诉写线程自己不再使用_ptr。读线程获取_ptr时，可能会获取到老的也可能获取到新的，无论哪种RCU都需要保证这块内存是有效的。重点在synchroize和dereference。synchroize会等待所有使用老的_ptr的线程dereference，对于新的_ptr使用者其不需要等待。这个问题说白了就是写线程如何知道old_ptr没有任何读线程在使用，可以安全地释放。</p>
<p>这个问题实际上在wait-free的各种实现中有好些解法，<a href="http://stackoverflow.com/questions/22263874/how-when-to-release-memory-in-wait-free-algorithms" target="_blank" rel="external">how-when-to-release-memory-in-wait-free-algorithms</a>这里有人总结了几种方法，例如Hazard pointers、Quiescence period based reclamation。</p>
<p>简单地使用引用计数智能指针是无法解决这个问题的，因为智能指针自己不是线程安全的，例如：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmp_ptr = _ptr      <span class="comment">// 1</span></span><br><span class="line">tmp_ptr-&gt;addRef()   <span class="comment">// 2</span></span><br><span class="line">use</span><br><span class="line">tmp_ptr-&gt;release()</span><br></pre></td></tr></table></figure></p>
<p>代码1/2行不是原子的，所以当取得tmp_ptr准备addRef时，tmp_ptr可能刚好被释放了。</p>
<p>Quiescence period based reclamation方法指的是读线程需要声明自己处于Quiescence period，也就是不使用_ptr的时候，当其使用_ptr的时候实际是进入了一个逻辑上的临界区，当所有读线程都不再使用_ptr的时候，写线程就可以对内存进行安全地释放。</p>
<p>本文正是描述了一种Quiescence period based reclamation实现。这个实现可以用于有一个写线程和多个读线程共用若干个数据的场景。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/多线程/">多线程</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/多线程/">多线程</a><a href="/tags/RCU/">RCU</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/12/23/大数据下的DistinctCount/" title="大数据下的DistinctCount" itemprop="url">大数据下的DistinctCount</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2016-12-23T03:53:53.000Z" itemprop="datePublished"> Published 2016-12-23</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>在数据库中，常常会有Distinct Count的操作，比如，查看每一选修课程的人数：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> course, <span class="keyword">count</span>(<span class="keyword">distinct</span> <span class="keyword">sid</span>)</span><br><span class="line"><span class="keyword">from</span> stu_table</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> course;</span><br></pre></td></tr></table></figure></p>
<h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><p>在大数据场景下，报表很重要一项是UV（Unique Visitor）统计，即某时间段内用户人数。例如，查看一周内app的用户分布情况，Hive中写HiveQL实现：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> app, <span class="keyword">count</span>(<span class="keyword">distinct</span> uid) <span class="keyword">as</span> uv</span><br><span class="line"><span class="keyword">from</span> log_table</span><br><span class="line"><span class="keyword">where</span> week_cal = <span class="string">'2016-03-27'</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> uv <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">20</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a>Pig</h2><p>大部分情况下，Hive的执行效率偏低，我更为偏爱Pig：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">-- all users define DISTINCT_COUNT(A, a) returns dist &#123;</span><br><span class="line">    B = foreach $A generate $a;</span><br><span class="line">    unique_B = distinct B;</span><br><span class="line">    C = group unique_B all;</span><br><span class="line">    $dist = foreach C generate SIZE(unique_B);</span><br><span class="line">&#125;</span><br><span class="line">A = load '/path/to/data' using PigStorage() as (app, uid);</span><br><span class="line">B = DISTINCT_COUNT(A, uid);</span><br><span class="line"></span><br><span class="line">-- &lt;app, users&gt;</span><br><span class="line">A = load '/path/to/data' using PigStorage() as (app, uid);</span><br><span class="line">B = distinct A;</span><br><span class="line">C = group B by app;</span><br><span class="line">D = foreach C generate group as app, COUNT($1) as uv;</span><br><span class="line">-- or</span><br><span class="line">D = foreach C generate group as app, SIZE($1) as uv;</span><br></pre></td></tr></table></figure></p>
<p>DataFu 为pig提供基数估计的UDF <figure class="highlight plain"><figcaption><span>Count：</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">```sql</span><br><span class="line">define HyperLogLogPlusPlus datafu.pig.stats.HyperLogLogPlusPlus();</span><br><span class="line">A = load &apos;/path/to/data&apos; using PigStorage() as (app, uid);</span><br><span class="line">B = group A by app;</span><br><span class="line">C = foreach B generate group as app, HyperLogLogPlusPlus($1) as uv;</span><br></pre></td></tr></table></figure></p>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><p>在Spark中，Load数据后通过RDD一系列的转换——map、distinct、reduceByKey进行Distinct Count：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rdd.map &#123; row =&gt; (row.app, row.uid) &#125;</span><br><span class="line">  .distinct()</span><br><span class="line">  .map &#123; line =&gt; (line._1, <span class="number">1</span>) &#125;</span><br><span class="line">  .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// or</span></span><br><span class="line">rdd.map &#123; row =&gt; (row.app, row.uid) &#125;</span><br><span class="line">  .distinct()</span><br><span class="line">  .mapValues&#123; _ =&gt; <span class="number">1</span> &#125;</span><br><span class="line">  .reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// or </span></span><br><span class="line">rdd.map &#123; row =&gt; (row.app, row.uid) &#125;</span><br><span class="line">  .distinct()</span><br><span class="line">  .map(_._1)</span><br><span class="line">  .countByValue()</span><br></pre></td></tr></table></figure></p>
<p>同时，Spark提供近似Distinct Count的API：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.map &#123; row =&gt; (row.app, row.uid) &#125;</span><br><span class="line">    .countApproxDistinctByKey(<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure></p>
<p>实现是基于HyperLogLog算法：</p>
<blockquote>
<p>The algorithm used is based on streamlib’s implementation of “HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm”, available here.</p>
</blockquote>
<p>或者，将Schema化的RDD转成DataFrame后，registerTempTable然后执行sql命令亦可：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF()</span><br><span class="line">df.registerTempTable(<span class="string">"app_table"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> appUsers = sqlContext.sql(<span class="string">"select app, count(distinct uid) as uv from app_table group by app"</span>)</span><br></pre></td></tr></table></figure></p>
<h2 id="Bitmap"><a href="#Bitmap" class="headerlink" title="Bitmap"></a>Bitmap</h2><h3 id="Bitmap介绍"><a href="#Bitmap介绍" class="headerlink" title="Bitmap介绍"></a>Bitmap介绍</h3><p>《编程珠玑》上是这样介绍bitmap的：</p>
<blockquote>
<p>Bitmap是一个十分有用的数据结构。所谓的Bitmap就是用一个bit位来标记某个元素对应的Value，而Key即是该元素。由于采用了Bit为单位来存储数据，因此在内存占用方面，可以大大节省。</p>
</blockquote>
<p>简而言之——用一个bit（0或1）表示某元素是否出现过，其在bitmap的位置对应于其index。《编程珠玑》给出了一个用bitmap做排序的例子：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Copyright (C) 1999 Lucent Technologies */</span></span><br><span class="line"><span class="comment">/* From 'Programming Pearls' by Jon Bentley */</span></span><br><span class="line"><span class="comment">/* bitsort.c -- bitmap sort from Column 1 * Sort distinct integers in the range [0..N-1] */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> BITSPERWORD 32</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SHIFT 5</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MASK 0x1F</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 10000000</span></span><br><span class="line"><span class="keyword">int</span> a[<span class="number">1</span> + N / BITSPERWORD];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; a[i &gt;&gt; SHIFT] |= (<span class="number">1</span> &lt;&lt; (i &amp; MASK)); &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">clr</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; a[i &gt;&gt; SHIFT] &amp;= ~(<span class="number">1</span> &lt;&lt; (i &amp; MASK)); &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; <span class="keyword">return</span> a[i &gt;&gt; SHIFT] &amp; (<span class="number">1</span> &lt;&lt; (i &amp; MASK)); &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">        clr(i);</span><br><span class="line">    <span class="comment">/* Replace above 2 lines with below 3 for word-parallel init  int top = 1 + N/BITSPERWORD;  for (i = 0; i &lt; top; i++)  a[i] = 0;  */</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;i) != EOF)</span><br><span class="line">        <span class="built_in">set</span>(i);</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">        <span class="keyword">if</span> (test(i))</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, i);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面代码中，用int的数组存储bitmap，对于每一个待排序的int数，其对应的index为其int值。</p>
<h3 id="Distinct-Count优化"><a href="#Distinct-Count优化" class="headerlink" title="Distinct Count优化"></a>Distinct Count优化</h3><ol>
<li>index生成</li>
</ol>
<p>为了使用bitmap做Distinct Count，首先需得到每个用户（uid）对应（在bitmap中）的index。有两种办法可以得到从1开始编号index表（与uid一一对应）：</p>
<ul>
<li>hash，但是要找到无碰撞且hash值均匀分布[1, +∞)区间的hash函数是非常困难的；<br>维护一张uid与index之间的映射表，并增量更新</li>
<li>比较两种方法，第二种方法更为简单可行。</li>
</ul>
<ol>
<li>UV计算</li>
</ol>
<p>在index生成完成后，RDD[(uid, V)]与RDD[(uid, index)]join得到index化的RDD。bitmap的开源实现有EWAH，采用RLE（Run Length Encoding）压缩，很好地解决了存储空间的浪费。Distinct Count计算转变成了求bitmap中1的个数：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// distinct count for rdd(not pair) and the rdd must be sorted in each partition</span></span><br><span class="line"><span class="comment">// 计算独立值的count</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinctCount</span></span>(rdd: <span class="type">RDD</span>[<span class="type">Int</span>]): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> bitmap = rdd.aggregate[<span class="type">EWAHCompressedBitmap</span>](<span class="keyword">new</span> <span class="type">EWAHCompressedBitmap</span>())(</span><br><span class="line">      (u: <span class="type">EWAHCompressedBitmap</span>, v: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">        u.set(v)</span><br><span class="line">        u</span><br><span class="line">      &#125;,</span><br><span class="line">      (u1: <span class="type">EWAHCompressedBitmap</span>, u2: <span class="type">EWAHCompressedBitmap</span>) =&gt; u1.or(u2)</span><br><span class="line">    )</span><br><span class="line">    bitmap.cardinality()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// the tuple_2 is the index</span></span><br><span class="line"><span class="comment">// 计算每个值对应的count</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupCount</span></span>[<span class="type">K</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Int</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">    <span class="keyword">val</span> grouped: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">EWAHCompressedBitmap</span>)] = rdd.combineByKey[<span class="type">EWAHCompressedBitmap</span>](</span><br><span class="line">      (v: <span class="type">Int</span>) =&gt; <span class="type">EWAHCompressedBitmap</span>.bitmapOf(v),</span><br><span class="line">      (c: <span class="type">EWAHCompressedBitmap</span>, v: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">        c.set(v)</span><br><span class="line">        c</span><br><span class="line">      &#125;,</span><br><span class="line">      (c1: <span class="type">EWAHCompressedBitmap</span>, c2: <span class="type">EWAHCompressedBitmap</span>) =&gt; c1.or(c2))</span><br><span class="line">    grouped.map(t =&gt; (t._1, t._2.cardinality()))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但是，在上述计算中，由于EWAHCompressedBitmap的set方法要求int值是升序的，也就是说RDD的每一个partition的index应是升序排列：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sort pair RDD by value</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortPairRDD</span></span>[<span class="type">K</span>](rdd: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Int</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">    rdd.mapPartitions(iter =&gt; &#123;</span><br><span class="line">      iter.toArray.sortWith((x, y) =&gt; x._2.compare(y._2) &lt; <span class="number">0</span>).iterator</span><br><span class="line">    &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>为了避免排序，可以为每一个uid生成一个bitmap，然后在Distinct Count时将bitmap进行or运算亦可：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rdd.reduceByKey(_ or _)</span><br><span class="line">    .mapValues(_._2.cardinality())</span><br></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Spark/">Spark</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Spark/">Spark</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/11/29/Scala的类型和反射机制/" title="Scala的类型和反射机制" itemprop="url">Scala的类型和反射机制</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2016-11-29T08:43:45.000Z" itemprop="datePublished"> Published 2016-11-29</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h1 id="Scala的反射机制"><a href="#Scala的反射机制" class="headerlink" title="Scala的反射机制"></a>Scala的反射机制</h1><ol>
<li><p>Manifest &amp; ClassManifest<br>Manifest是在编译时捕捉的，编码了“捕捉时”所致的类型信息。然后就可以在运行时检查和使用类型信息，但是manifest只能捕捉当Manifest被查找时在隐式作用域里的类型。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>[<span class="type">A</span> : <span class="type">ClassManifest</span>](x:<span class="type">Array</span>[<span class="type">A</span>]) = <span class="type">Array</span>(x(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>ClassTag &amp; TypeTag</p>
</li>
</ol>
<ul>
<li><p>ClassTag[T]保存了被泛型擦除后的原始类型T,提供给运行时的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">mkArray</span></span>[<span class="type">T</span> : <span class="type">ClassTag</span>](elems: <span class="type">T</span>*) = <span class="type">Array</span>[<span class="type">T</span>](elems: _*)</span><br><span class="line">mkArray: [<span class="type">T</span>](elems: <span class="type">T</span>*)(<span class="keyword">implicit</span> evidence$<span class="number">1</span>: scala.reflect.<span class="type">ClassTag</span>[<span class="type">T</span>])<span class="type">Array</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>TypeTag则保存所有具体的类型</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.reflect.runtime.universe._</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">paramInfo</span></span>[<span class="type">T</span>](x: <span class="type">T</span>)(<span class="keyword">implicit</span> tag: <span class="type">TypeTag</span>[<span class="type">T</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> targs = tag.tpe <span class="keyword">match</span> &#123; <span class="keyword">case</span> <span class="type">TypeRef</span>(_, _, args) =&gt; args &#125;</span><br><span class="line">  println(<span class="string">s"type of <span class="subst">$x</span> has type arguments <span class="subst">$targs</span>"</span>)</span><br><span class="line">&#125;</span><br><span class="line">scala&gt; paramInfo(<span class="number">42</span>)</span><br><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">of</span> 42 <span class="title">has</span> <span class="title">type</span> <span class="title">arguments</span> <span class="title">List</span>(<span class="params"></span>)</span></span><br><span class="line">scala&gt; paramInfo(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="class"><span class="keyword">type</span> <span class="title">of</span> <span class="title">List</span>(<span class="params">1, 2</span>) <span class="title">has</span> <span class="title">type</span> <span class="title">arguments</span> <span class="title">List</span>(<span class="params"><span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看到，获取到的类型是具体的类型，而不是被擦除后的类型List(Any)</p>
<p>scala在2.10里却用TypeTag替代了Manifest，用ClassTag替代了ClassManifest，原因是在路径依赖类型中，Manifest存在问题：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>&#123;<span class="class"><span class="keyword">class</span> <span class="title">Bar</span>&#125;</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">m</span></span>(f: <span class="type">Foo</span>)(b: f.<span class="type">Bar</span>)(<span class="keyword">implicit</span> ev: <span class="type">Manifest</span>[f.<span class="type">Bar</span>]) = ev</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> f1 = <span class="keyword">new</span> <span class="type">Foo</span>;<span class="keyword">val</span> b1 = <span class="keyword">new</span> f1.<span class="type">Bar</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> f2 = <span class="keyword">new</span> <span class="type">Foo</span>;<span class="keyword">val</span> b2 = <span class="keyword">new</span> f2.<span class="type">Bar</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ev1 = m(f1)(b1)</span><br><span class="line">ev1: <span class="type">Manifest</span>[f1.<span class="type">Bar</span>] = <span class="type">Foo</span>@<span class="number">681e731</span>c.<span class="keyword">type</span>#<span class="type">Foo</span>$<span class="type">Bar</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> ev2 = m(f2)(b2)</span><br><span class="line">ev2: <span class="type">Manifest</span>[f2.<span class="type">Bar</span>] = <span class="type">Foo</span>@<span class="number">3e50039</span>c.<span class="keyword">type</span>#<span class="type">Foo</span>$<span class="type">Bar</span></span><br><span class="line"></span><br><span class="line">scala&gt; ev1 == ev2 <span class="comment">// they should be different, thus the result is wrong</span></span><br><span class="line">res28: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>ev1 不应该等于 ev2 的，因为其依赖路径（外部实例）是不一样的。<br>还有其他因素，所以在2.10版本里，使用 TypeTag 替代了 Manifest<br>TypeTag:由编辑器生成,只能通过隐式参数或者上下文绑定获取<br>可以有两种方式获取:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> scala.reflect.runtime.universe._</span><br><span class="line"><span class="keyword">import</span> scala.reflect.runtime.universe._</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用typeTag</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">getTypeTag</span></span>[<span class="type">T</span>:<span class="type">TypeTag</span>](a:<span class="type">T</span>) = typeTag[<span class="type">T</span>]</span><br><span class="line">getTypeTag: [<span class="type">T</span>](a: <span class="type">T</span>)(<span class="keyword">implicit</span> evidence$<span class="number">1</span>: reflect.runtime.universe.<span class="type">TypeTag</span>[<span class="type">T</span>])reflect.runtime.universe.<span class="type">TypeTag</span>[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用implicitly 等价的 </span></span><br><span class="line"><span class="comment">//scala&gt;def getTypeTag[T:TypeTag](a:T) = implicitly[TypeTag[T]]</span></span><br><span class="line"></span><br><span class="line">scala&gt; getTypeTag(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">res0: reflect.runtime.universe.<span class="type">TypeTag</span>[<span class="type">List</span>[<span class="type">Int</span>]] = <span class="type">TypeTag</span>[<span class="type">List</span>[<span class="type">Int</span>]]</span><br></pre></td></tr></table></figure></p>
<p>通过TypeTag的tpe方法获得需要的Type(如果不是从对象换取Type 而是从class中获得 可以直接用 typeOf[类名])</p>
<ol>
<li><p>反射获取TypeTag和ClassTag<br><a href="http://www.programcreek.com/java-api-examples/index.php?api=scala.reflect.ClassTag" target="_blank" rel="external">JavaCodeExample</a><br>String———-&gt;Calss————–&gt;Manifest——–&gt;TypeTag<br>Class.forName    ManifestFactory.classType scala.reflect.runtime.universe.manifestToTypeTag</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scala.reflect.runtime.universe</span><br><span class="line"><span class="keyword">import</span> scala.reflect.<span class="type">ManifestFactory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> className = <span class="string">"java.lang.String"</span></span><br><span class="line"><span class="keyword">val</span> mirror = universe.runtimeMirror(getClass.getClassLoader)</span><br><span class="line"><span class="keyword">val</span> cls = <span class="type">Class</span>.forName(className)</span><br><span class="line"><span class="keyword">val</span> t = universe.manifestToTypeTag(mirror, <span class="type">ManifestFactory</span>.classType(cls))</span><br></pre></td></tr></table></figure>
</li>
<li><p>classOf与getClass方法的差异</p>
</li>
</ol>
<p>getClass 方法得到的是 Class[A]的某个子类，而 classOf[A] 得到是正确的 Class[A]，但是去比较的话，这两个类型是equals为true的</p>
<p>classOf获取运行时的类型。classOf[T] 相当于 java中的T.class; 而getClass:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> listClass = classOf[<span class="type">List</span>[_]]</span><br><span class="line">   * <span class="comment">// listClass is java.lang.Class[List[_]] = class scala.collection.immutable.List</span></span><br><span class="line"><span class="keyword">val</span> mapIntString = classOf[<span class="type">Map</span>[<span class="type">Int</span>,<span class="type">String</span>]]</span><br><span class="line">   * <span class="comment">// mapIntString is java.lang.Class[Map[Int,String]] = interface scala.collection.immutable.Map</span></span><br><span class="line">   * &#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="class"><span class="keyword">class</span>  <span class="title">A</span></span></span><br><span class="line">scala&gt; <span class="keyword">val</span> a = <span class="keyword">new</span> <span class="type">A</span></span><br><span class="line"></span><br><span class="line">scala&gt; a.getClass</span><br><span class="line">res2: <span class="type">Class</span>[_ &lt;: <span class="type">A</span>] = <span class="class"><span class="keyword">class</span> <span class="title">A</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; classOf[<span class="type">A</span>]</span><br><span class="line">res3: <span class="type">Class</span>[<span class="type">A</span>] = <span class="class"><span class="keyword">class</span> <span class="title">A</span></span></span><br><span class="line"></span><br><span class="line">scala&gt; a.getClass  == classOf[<span class="type">A</span>]</span><br><span class="line">res13: <span class="type">Boolean</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<p>这种细微的差别，体现在类型赋值时，因为java里的 Class[T]是不支持协变的，所以无法把一个 Class[_ &lt; : A] 赋值给一个 Class[A]<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> c:<span class="type">Class</span>[<span class="type">A</span>] = a.getClass</span><br><span class="line">&lt;console&gt;:<span class="number">9</span>: error: <span class="class"><span class="keyword">type</span> <span class="title">mismatch</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>类(class)与类型(type)是两个不一样的概念</p>
<p>(在java里因为早期一直使用class表达type，并且现在也延续这样的习惯)；类型(type)比类(class)更”具体”，任何数据都有类型。类是面向对象系统里对同一类数据的抽象，在没有泛型之前，类型系统不存在高阶概念，直接与类一一映射，而泛型出现之后，就不在一一映射了。比如定义class List[T] {}, 可以有List[Int] 和 List[String]等具体类型，它们的类是同一个List，但类型则根据不同的构造参数类型而不同。</p>
<p>类型一致的对象它们的类也是一致的，反过来，类一致的，其类型不一定一致。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; classOf[<span class="type">List</span>[<span class="type">Int</span>]] == classOf[<span class="type">List</span>[<span class="type">String</span>]]</span><br><span class="line">res16: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">scala&gt; typeOf[<span class="type">List</span>[<span class="type">Int</span>]] == typeOf[<span class="type">List</span>[<span class="type">String</span>]]</span><br><span class="line">res17: <span class="type">Boolean</span> = <span class="literal">false</span></span><br></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Scala/">Scala</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Scala/">Scala</a><a href="/tags/反射/">反射</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2016/11/29/JVM源码分析系列/" title="JVM源码分析系列" itemprop="url">JVM源码分析系列</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2016-11-29T08:43:45.000Z" itemprop="datePublished"> Published 2016-11-29</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>JVM源码分析系列</p>
<ol>
<li><p>不保证顺序的Class.getMethods<br><a href="http://mp.weixin.qq.com/s?__biz=MzIzNjI1ODc2OA==&amp;mid=2650886863&amp;idx=1&amp;sn=1cc34f397e0c62d1126560ff43f30069&amp;chksm=f32f6670c458ef66d56f51187547c4aff7b62038ea941c8048b014e6a1e2d2e9252369397866&amp;scene=0#rd" target="_blank" rel="external">JVM源码分析之不保证顺序的Class.getMethods</a></p>
</li>
<li><p>Metaspace<br><a href="http://mp.weixin.qq.com/s?__biz=MzIzNjI1ODc2OA==&amp;mid=2650886860&amp;idx=1&amp;sn=f8bc6ab03d7a07022c86bf726209b17c&amp;chksm=f32f6673c458ef657358068a8aea4664d3cbc8a488e6bfd6445c0743140f6fc8bcf9649298b7&amp;scene=0#wechat_redirect" target="_blank" rel="external">JVM源码分析之Metaspace解密</a></p>
</li>
</ol>
<p>metaspace，顾名思义，元数据空间，专门用来存元数据的，它是jdk8里特有的数据结构用来替代perm</p>
<ul>
<li>为什么会有metaspace<br>如果perm设置太小了，系统运行过程中就容易出现内存溢出，设置大了又总感觉浪费，尽管不会实质分配这么大的物理内存。基于这么一个可能的原因，于是metaspace出现了，希望内存的管理不再受到限制，也不要怎么关注元数据这块的OOM问题</li>
<li>metaspace的组成<ul>
<li>Klass Metaspace Klass Metaspace就是用来存klass的，klass是我们熟知的class文件在jvm里的运行时数据结构，不过有点要提的是我们看到的类似A.class其实是存在heap里的，是java.lang.Class的一个对象实例。</li>
<li>NoKlass Metaspace  NoKlass Metaspace专门来存klass相关的其他的内容，比如method，constantPool等，这块内存是由多块内存组合起来的，所以可以认为是不连续的内存块组成的。</li>
</ul>
</li>
</ul>
<p>Klass Metaspace和NoKlass Mestaspace都是所有classloader共享的，所以类加载器们要分配内存，但是每个类加载器都有一个SpaceManager，来管理属于这个类加载的内存小块。</p>
<ol>
<li>JVM源码分析之String.intern()导致的YGC不断变长<br><a href="http://mp.weixin.qq.com/s?__biz=MzIzNjI1ODc2OA==&amp;mid=2650886867&amp;idx=1&amp;sn=e4433f7068357b0f9ed283b607fa50e6&amp;chksm=f32f666cc458ef7a0132c6dfb74bc53626b47d884db7ae1b29a41bea3527e416c87c71c49fbc&amp;scene=0#rd" target="_blank" rel="external">JVM源码分析之String.intern()导致的YGC不断变长</a></li>
</ol>
<p>在JVM里存在一个叫做StringTable的数据结构，这个数据结构是一个Hashtable，在我们调用String.intern的时候其实就是先去这个StringTable里查找是否存在一个同名的项，如果存在就直接返回对应的对象，否则就往这个table里插入一项，指向这个String对象，那么再下次通过intern再来访问同名的String对象的时候，就会返回上次插入的这一项指向的String对象</p>
<p>YGC的时间长短和扫描StringTable有关,如果StringTable非常庞大，那YGC过程扫描的时间也会变长</p>
<p>YGC过程不会对StringTable做清理，这也就是我们demo里的情况会让Stringtable越来越大，因为到目前为止还只看到YGC过程，但是在Full GC或者CMS GC过程会对StringTable做清理</p>
<ol>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzIzNjI1ODc2OA==&amp;mid=403257534&amp;idx=1&amp;sn=2015e011c50c0a9107a48aa60a4adb78&amp;scene=21#wechat_redirect" target="_blank" rel="external">如何定位消耗CPU最多的线程</a></li>
</ol>
<p>步骤</p>
<pre><code>- 使用top -Hp &lt;pid&gt; 查看进程所有线程的CPU消耗情况

- 使用jstack &lt;pid&gt; 查看各个线程栈
</code></pre><p> 5.<a href="http://mp.weixin.qq.com/s?__biz=MzIzNjI1ODc2OA==&amp;mid=403191974&amp;idx=1&amp;sn=def1d96a9abbfa22fd1aeedd254af5c7&amp;scene=21#wechat_redirect" target="_blank" rel="external">JVM源码分析之Object.wait/notify(All)完全解读</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> Thread()&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">synchronized</span>(lock) &#123;</span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				lock.wait();</span><br><span class="line">			&#125; <span class="keyword">catch</span>(InterruptedException e) &#123;</span><br><span class="line">				e.printStackTrace();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;.start();</span><br><span class="line"><span class="keyword">new</span> Thread()&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">		<span class="keyword">synchronized</span>(lock) &#123;</span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				lock.notify();</span><br><span class="line">			&#125; </span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;.start();</span><br></pre></td></tr></table></figure>
<ul>
<li>为何要加synchronized锁<br>从实现上来说，这个锁至关重要，正因为这把锁，才能让整个wait/notify玩转起来</li>
<li>wait方法执行后未退出同步块，其他线程如何进入同步块<br>因为在wait处理过程中会临时释放同步锁，不过需要注意的是当某个线程调用notify唤起了这个线程的时候，在wait方法退出之前会重新获取这把锁，只有获取了这把锁才会继续执行</li>
<li>为什么wait方法可能抛出InterruptedException异常<br>这个异常大家应该都知道，当我们调用了某个线程的interrupt方法时，对应的线程会抛出这个异常，wait方法也不希望破坏这种规则，因此就算当前线程因为wait一直在阻塞，当某个线程希望它起来继续执行的时候，它还是得从阻塞态恢复过来，因此wait方法被唤醒起来的时候会去检测这个状态，当有线程interrupt了它的时候，它就会抛出这个异常从阻塞状态恢复过来。</li>
<li>被notify(All)的线程有规律吗<br>这里要分情况： <em> 如果是通过notify来唤起的线程，那先进入wait的线程会先被唤起来 </em> 如果是通过nootifyAll唤起的线程，默认情况是最后进入的会先被唤起来，即LIFO的策略</li>
<li>notify执行之后立马唤醒线程吗<br>hotspot里真正的实现是退出同步块的时候才会去真正唤醒对应的线程，</li>
<li>notifyAll是怎么实现全唤起的<br>以notifyAll的实现是调用notify的线程在退出其同步块的时候唤醒起最后一个进入wait状态的线程，然后这个线程退出同步块的时候继续唤醒其倒数第二个进入wait状态的线程，依次类推</li>
<li>wait的线程是否会影响load<br>当线程进入到wait状态的时候其实是会放弃cpu的，也就是说这类线程是不会占用cpu资源</li>
</ul>
<ol>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzIzNjI1ODc2OA==&amp;mid=403241108&amp;idx=1&amp;sn=cd58280b00d0a8ef93b8eb78bfed4457&amp;scene=21#wechat_redirect" target="_blank" rel="external">JVM源码分析之FinalReference完全解读</a></li>
</ol>
<p>override finalizer的类对象称为f对象。</p>
<p>Finalizer的客观评价</p>
<ul>
<li>Finalizer其实是实现了析构函数的概念，我们在对象被回收前可以执行一些『收拾性』的逻辑，应该说是一个特殊场景的补充，但是这种概念的实现给我们的f对象生命周期以及gc等带来了一些影响： </li>
<li>f对象因为Finalizer的引用而变成了一个临时的强引用，即使没有其他的强引用了，还是无法立即被回收<br>f对象至少经历两次GC才能被回收，因为只有在FinalizerThread执行完了f对象的finalize方法的情况下才有可能被下次gc回收，而有可能期间已经经历过多次gc了，但是一直还没执行f对象的finalize方法</li>
<li>cpu资源比较稀缺的情况下FinalizerThread线程有可能因为优先级比较低而延迟执行f对象的finalize方法</li>
<li>因为f对象的finalize方法迟迟没有执行，有可能会导致大部分f对象进入到old分代，此时容易引发old分代的gc，甚至fullgc，gc暂停时间明显变长</li>
<li>f对象的finalize方法被调用了，但是这个对象其实还并没有被回收，虽然可能在不久的将来会被回收</li>
</ul>
<ol>
<li>不可逆的类初始化过程<br>定义一个类的时候，可能有静态变量，可能有静态代码块，这些逻辑编译之后会封装到一个叫做clinit的方法里。<br>clinit方法在我们第一次主动使用这个类的时候会触发执行，比如我们访问这个类的静态方法或者静态字段就会触发执行clinit，但是这个过程是不可逆的，也就是说当我们执行一遍之后再也不会执行了，如果在执行这个方法过程中出现了异常没有被捕获，那这个类将永远不可用。即使抛出异常，被catch住，也依然会被JVM标记为error标记。<br>如果clinit执行失败了，抛了一个未被捕获的异常，那将这个类的状态设置为initialization_error,并且无法再恢复，因为jvm会认为你这次初始化失败了，下次肯定也是失败的，为了防止不断抛这种异常，所以做了一个缓存处理，不是每次都再去执行clinit，因此大家要特别注意，类的初始化过程可千万不能出错，出错就可能只能重启了哦。</li>
</ol>
<p>8.JVM源码分析之jstat工具原理完全解读</p>
<ul>
<li>jstat如何获取到这些变量的值</li>
</ul>
<p>变量值显然是从目标进程里获取来的，但是是怎样来的？local socket还是memory share？其实是从一个共享文件里来的，这个文件叫PerfData，主要指的是/tmp/hsperfdata_<user>/<pid>这个文件</pid></user></p>
<ul>
<li>PerfData文件<ul>
<li>文件创建<br>这个文件是否存在取决于两个参数，一个UsePerfData，另一个是PerfDisableSharedMem，如果设置了-XX:+PerfDisableSharedMem或者-XX:-UsePerfData，那这个文件是不会存在的，默认情况下PerfDisableSharedMem是关闭的，UsePerfData是打开的，所以默认情况下PerfData文件是存在的。</li>
<li>文件删除<br>那这个文件什么时候删除？正常情况下当进程退出的时候会自动删除，但是某些极端情况下，比如kill -9，这种信号jvm是不能捕获的，所以导致进程直接退出了，而没有做一些收尾性的工作，这个时候你会发现进程虽然没了，但是这个文件其实还是存在的。在当前用户接下来的任何一个java进程(比如说我们执行jps)起来的时候会去做一个判断，遍历/tmp/hsperfdata_<user>下的进程文件，挨个看进程是不是还存在，如果不存在了就直接删除该文件，判断是否存在的具体操作其实就是发一个kill -0的信号看是否有异常。</user></li>
<li>文件更新<br>由于这个文件是通过mmap的方式映射到了内存里，而jstat是直接通过DirectByteBuffer的方式从PerfData里读取的，所以只要内存里的值变了，那我们从jstat看到的值就会发生变化，内存里的值什么时候变，取决于-XX:PerfDataSamplingInterval这个参数，默认是50ms，也就是说50ms更新一次值，基本上可以认为是实时的了。</li>
</ul>
</li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Java/">Java</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Java/">Java</a><a href="/tags/JVM/">JVM</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/page/4/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/6/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github Card</p>
<div class="github-card" data-github="slamke" data-width="220" data-height="119" data-theme="medium">
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>
  </div>



  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/HDFS/" title="HDFS">HDFS<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Hadoop/" title="Hadoop">Hadoop<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/Hive/" title="Hive">Hive<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/Java/" title="Java">Java<sup>12</sup></a></li>
		  
		
		  
			<li><a href="/categories/Kafka/" title="Kafka">Kafka<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/ML/" title="ML">ML<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/Python/" title="Python">Python<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/REST/" title="REST">REST<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/Reactor/" title="Reactor">Reactor<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Scala/" title="Scala">Scala<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/Spark/" title="Spark">Spark<sup>29</sup></a></li>
		  
		
		  
			<li><a href="/categories/Spring/" title="Spring">Spring<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/Tomcat/" title="Tomcat">Tomcat<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Web/" title="Web">Web<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/多线程/" title="多线程">多线程<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/数据仓库/" title="数据仓库">数据仓库<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/算法/" title="算法">算法<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/重构/" title="重构">重构<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Spark/" title="Spark">Spark<sup>26</sup></a></li>
			
		
			
				<li><a href="/tags/Java/" title="Java">Java<sup>11</sup></a></li>
			
		
			
				<li><a href="/tags/Web/" title="Web">Web<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/Spring/" title="Spring">Spring<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/Hive/" title="Hive">Hive<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/Hadoop/" title="Hadoop">Hadoop<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/Scala/" title="Scala">Scala<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/多线程/" title="多线程">多线程<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Cookie/" title="Cookie">Cookie<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Spark-Streaming/" title="Spark Streaming">Spark Streaming<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Yarn/" title="Yarn">Yarn<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Session/" title="Session">Session<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/算法/" title="算法">算法<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Spark2-0/" title="Spark2.0">Spark2.0<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/ML/" title="ML">ML<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Junit/" title="Junit">Junit<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/REST/" title="REST">REST<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/java/" title="java">java<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/UDF/" title="UDF">UDF<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/HDFS/" title="HDFS">HDFS<sup>2</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="http://slamke.blogspot.com/" target="_blank" title="我的博客">我的博客</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">Weibo</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=null&verifier=&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Larry Page in Google. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		<a href="https://github.com/slamke" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:sunke3296@gmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2018 
		
		<a href="/about" target="_blank" title="Sun Ke">Sun Ke</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
