
 <!DOCTYPE HTML>
<html >
<head>
  <meta charset="UTF-8">
  
    <title>雁渡寒潭 风吹疏竹</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Sun Ke">
    

    
    <meta name="description" content="人生不止眼前的苟且">
<meta property="og:type" content="website">
<meta property="og:title" content="雁渡寒潭 风吹疏竹">
<meta property="og:url" content="slamke.github.io/page/5/index.html">
<meta property="og:site_name" content="雁渡寒潭 风吹疏竹">
<meta property="og:description" content="人生不止眼前的苟且">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="雁渡寒潭 风吹疏竹">
<meta name="twitter:description" content="人生不止眼前的苟且">

    
    <link rel="alternative" href="/atom.xml" title="雁渡寒潭 风吹疏竹" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="雁渡寒潭 风吹疏竹" title="雁渡寒潭 风吹疏竹"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="雁渡寒潭 风吹疏竹">雁渡寒潭 风吹疏竹</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
						<li><a href="/about">About</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:slamke.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/28/Hadoop的安全机制/" title="Hadoop的安全机制" itemprop="url">Hadoop的安全机制</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-03-28T09:31:00.000Z" itemprop="datePublished"> Published 2017-03-28</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>Hadoop Kerberos安全机制介绍</p>
<p>参考: <a href="http://dongxicheng.org/mapreduce/hadoop-kerberos-introduction/" target="_blank" rel="external">http://dongxicheng.org/mapreduce/hadoop-kerberos-introduction/</a></p>
<p>在Hadoop1.0.0或者CDH3版本后，加入了Kerberos认证机制。使得集群中的节点就是它们所宣称的，是信赖的。Kerberos可以将认证的密钥在集群部署时事先放到可靠的节点上。集群运行时，集群内的节点使用密钥得到认证。只有被认证过节点才能正常使用。企图冒充的节点由于没有事先得到的密钥信息，无法与集群内部的节点通信。防止了恶意的使用或篡改Hadoop集群的问题，确保了Hadoop集群的可靠安全。</p>
<ul>
<li>解决服务器到服务器的认证<br>由于kerberos对集群里的所有机器都分发了keytab，相互之间使用密钥进行通信，确保不会冒充服务器的情况。集群中的机器就是它们所宣称的，是可靠的。<br>防止了用户伪装成Datanode，Tasktracker，去接受JobTracker，Namenode的任务指派。</li>
<li>解决client到服务器的认证<br>Kerberos对可信任的客户端提供认证，确保他们可以执行作业的相关操作。防止用户恶意冒充client提交作业的情况。<br>用户无法伪装成其他用户入侵到一个HDFS 或者MapReduce集群上<br>用户即使知道datanode的相关信息，也无法读取HDFS上的数据<br>用户无法发送对于作业的操作到JobTracker上</li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Hadoop/">Hadoop</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Hadoop/">Hadoop</a><a href="/tags/安全机制/">安全机制</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/28/SparkSQL有必要坐下来聊聊Join/" title="SparkSQL有必要坐下来聊聊Join" itemprop="url">SparkSQL有必要坐下来聊聊Join</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-03-28T03:15:46.000Z" itemprop="datePublished"> Published 2017-03-28</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>转自 <a href="http://hbasefly.com/2017/03/19/sparksql-basic-join/" target="_blank" rel="external">http://hbasefly.com/2017/03/19/sparksql-basic-join/</a></p>
<p>HashJoin</p>
<p>Broadcast Hash Join</p>
<p>Shuffle Hash Join</p>
<p>Sort-Merge Join</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Spark/">Spark</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Spark/">Spark</a><a href="/tags/SparkSQL/">SparkSQL</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/19/Java的通用I-O-API设计/" title="Java的通用I/O API设计" itemprop="url">Java的通用I/O API设计</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-03-19T12:47:53.000Z" itemprop="datePublished"> Published 2017-03-19</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p><a href="https://github.com/oldratlee/translations/blob/master/generic-io-api-in-java-and-api-design/README.md" target="_blank" rel="external">博客</a></p>
<p><a href="https://github.com/oldratlee/io-api/wiki/java-api-design-exercise" target="_blank" rel="external">wiki说明</a></p>
<p><a href="https://github.com/oldratlee/io-api" target="_blank" rel="external">代码库</a></p>
<p><a href="http://lcsd05.cs.tamu.edu/slides/keynote.pdf" target="_blank" rel="external">How to Design a Good API and Why it Matters(by Joshua Bloch)</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Java/">Java</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Java/">Java</a><a href="/tags/API/">API</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/15/作为Scala语法糖的设计模式/" title="作为Scala语法糖的设计模式" itemprop="url">作为Scala语法糖的设计模式</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-03-15T07:49:22.000Z" itemprop="datePublished"> Published 2017-03-15</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>转载: <a href="http://zhangyi.farbox.com/post/designthinking/design-patterns-with-scala-syntax-sugar" target="_blank" rel="external">http://zhangyi.farbox.com/post/designthinking/design-patterns-with-scala-syntax-sugar</a></p>
<p>Scala算是一门博采众家之长的语言，兼具OO与FP的特性，若使用恰当，可以更好地将OO与FP的各自优势发挥到极致；然而问题也随之而来，倘若过分地夸大OO特性，Scala就变成了一门精简版的Java，写出的是没有Scala Style的拙劣代码；倘若过分追求FP的不变性等特性，因为Scala在类型系统以及Monad实现的繁琐性，又可能导致代码变得复杂，不易阅读，反而得不偿失。</p>
<p>看来，赋予程序员选择的自由，有时候未必是好事！</p>
<p>在OO世界里，设计模式曾经风靡全世界，你不懂设计模式，都不好意思说自己是程序员。现在呢？说你懂设计模式，倒显得你逼格低了，心里鄙视：“这年头谁还用设计模式，早过时了！”程序员心中的鄙视链开始加成，直接失血二十格。</p>
<p>其实什么事情都得辩证来看！设计模式对OO设计的推进作用不容忽视，更不容轻视。我只是反对那种为了“模式”而“模式”的僵化思想，如果没有明白设计模式的本质思想，了解根本的设计原理，设计模式无非就是花拳绣腿罢了。当然，在FP世界里，设计模式开始变味开始走形，但诸多模式的本质，例如封装、抽象，仍然贯穿其中，不过是表达形式迥然而已罢了。</p>
<p>在混合了OO与FP的Scala语言中，我们来观察设计模式的实现，会非常有趣。Pavel Fatin有篇博客Design Patterns in Scala将Java设计模式与Scala进行了对比，值得一读。我这里想借用他的案例，然后从另一个角度来俯瞰设计模式。</p>
<p>在Pavel Fatin比较的设计模式中，部分模式在Scala中不过是一种语法糖（Syntax Sugar），包括：</p>
<ul>
<li>Factory Method</li>
<li>Lazy Initialization</li>
<li>Singleton</li>
<li>Adapter</li>
<li>Value Object</li>
<li>Factory Method</li>
</ul>
<p>文中给出的Factory Method模式，准确地说其实是静态工厂模式，它并不在GOF 23种模式之列，但作为对复杂创建逻辑的一种封装，常常被开发人员使用。站在OCP（开放封闭原则）的角度讲，该模式对扩展不是开放的，但对于修改而言，却是封闭的。如果创建逻辑发生了变化，可以保证仅修改该静态工厂方法一处。同时，该模式还可以极大地简化对象创建的API。</p>
<p>在Scala中，通过引入伴生对象（Companion Object）来简化静态工厂方法，语法更加干净，体现了Scala精简的设计哲学。即使不是要使用静态工厂，我们也常常建议为Scala类定义伴生对象，尤其是在DSL上下文中，更是如此，因为这样可以减少new关键字对代码的干扰。</p>
<h2 id="Lazy-Initialization"><a href="#Lazy-Initialization" class="headerlink" title="Lazy Initialization"></a>Lazy Initialization</h2><p>lazy修饰符在Scala中有更深远的涵义，例如牵涉到所谓严格（Strictness）函数与非严格（Non-strictness）函数。在Scala中，若未明确声明，所有函数都是严格求值的，即函数会立即对它的参数进行求值。而如果对val变量添加lazy修饰符，则Scala会延迟对该变量求值，直到它第一次被引用时。如果要定义非严格函数，可以将函数设置为by name参数。</p>
<p>scala的lazy修饰符常常被用作定义一些消耗资源的变量。这些资源在初始化时并不需要，只有在调用某些方法时，才需要准备好这些资源。例如在Spark SQL的QeuryExecution类中，包括optimizedPlan、sparkPlan、executedPlan以及toRdd等，都被定义为lazy val：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QueryExecution</span>(<span class="params">val sparkSession: <span class="type">SparkSession</span>, val logical: <span class="type">LogicalPlan</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> analyzed: <span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">    <span class="type">SparkSession</span>.setActiveSession(sparkSession)</span><br><span class="line">    sparkSession.sessionState.analyzer.execute(logical)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> withCachedData: <span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">    assertAnalyzed()</span><br><span class="line">    assertSupported()</span><br><span class="line">    sparkSession.sharedState.cacheManager.useCachedData(analyzed)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> optimizedPlan: <span class="type">LogicalPlan</span> = sparkSession.sessionState.optimizer.execute(withCachedData)</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> sparkPlan: <span class="type">SparkPlan</span> = &#123;</span><br><span class="line">    <span class="type">SparkSession</span>.setActiveSession(sparkSession)</span><br><span class="line">    planner.plan(<span class="type">ReturnAnswer</span>(optimizedPlan)).next()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> executedPlan: <span class="type">SparkPlan</span> = prepareForExecution(sparkPlan)</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> toRdd: <span class="type">RDD</span>[<span class="type">InternalRow</span>] = executedPlan.execute()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样设计有一个好处是，当程序在执行到这些步骤时，并不会被马上执行，从而使得初始化QueryExecution变得更快。只有在需要时，这些变量对应的代码才会执行。这也是延迟加载的涵义。</p>
<h2 id="Singleton-Pattern"><a href="#Singleton-Pattern" class="headerlink" title="Singleton Pattern"></a>Singleton Pattern</h2><p>C#提供了静态类的概念，但Java没有，而Scala则通过引入Object弥补了Java的这一缺失，而且从语义上讲，似乎比静态类（Static Class）更容易让人理解。</p>
<p>Object可以派生自多个trait。例如派生自App trait，就可直接享有main函数的福利。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">App</span> <span class="keyword">extends</span> <span class="title">DelayedInit</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">    <span class="keyword">this</span>._args = args</span><br><span class="line">    <span class="keyword">for</span> (proc &lt;- initCode) proc()</span><br><span class="line">    <span class="keyword">if</span> (util.<span class="type">Properties</span>.propIsSet(<span class="string">"scala.time"</span>)) &#123;</span><br><span class="line">      <span class="keyword">val</span> total = currentTime - executionStart</span><br><span class="line">      <span class="type">Console</span>.println(<span class="string">"[total "</span> + total + <span class="string">"ms]"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Main</span> <span class="keyword">extends</span> <span class="title">App</span></span></span><br></pre></td></tr></table></figure>
<p>继承多个trait的好处是代码复用。我们可以将许多小粒度方法的实现定义在多个trait中。这些方法如果被类继承，则成为实例方法，如果被Object继承，则变成了线程安全的静态方法（因为继承trait的实现就是一个mixin）。多么奇妙！所以很多时候，我们会尽量保证Obejct的短小精悍，然后将许多逻辑放到trait中。当你看到如下代码时，其实不必惊讶：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Main</span> <span class="keyword">extends</span> <span class="title">App</span> </span></span><br><span class="line">  <span class="keyword">with</span> <span class="type">InitHook</span></span><br><span class="line">  <span class="keyword">with</span> <span class="type">ShutdownHook</span></span><br><span class="line">  <span class="keyword">with</span> <span class="type">ActorSystemProvider</span></span><br><span class="line">  <span class="keyword">with</span> <span class="type">ScheduledTaskSupport</span></span><br></pre></td></tr></table></figure></p>
<p>这种小粒度的trait既可以保证代码的复用，也有助于职责分离，还有利于测试。真是再好不过了！</p>
<h2 id="Adapter-Pattern"><a href="#Adapter-Pattern" class="headerlink" title="Adapter Pattern"></a>Adapter Pattern</h2><p>隐式转换当然可以用作Adapter。在Scala中，之所以可以更好地调用Java库，隐式转换功不可没。从语法上看，隐式转换比C#提供的扩展方法更强大，适用范围更广。</p>
<p>Pavel Fatin给出了日志转换的Adapter案例：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Log</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">warning</span></span>(message: <span class="type">String</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">error</span></span>(message: <span class="type">String</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Logger</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">log</span></span>(level: <span class="type">Level</span>, message: <span class="type">String</span>) &#123; <span class="comment">/* ... */</span> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">implicit</span> <span class="class"><span class="keyword">class</span> <span class="title">LoggerToLogAdapter</span>(<span class="params">logger: <span class="type">Logger</span></span>) <span class="keyword">extends</span> <span class="title">Log</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">warning</span></span>(message: <span class="type">String</span>) &#123; logger.log(<span class="type">WARNING</span>, message) &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">error</span></span>(message: <span class="type">String</span>) &#123; logger.log(<span class="type">ERROR</span>, message) &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> log: <span class="type">Log</span> = <span class="keyword">new</span> <span class="type">Logger</span>()</span><br></pre></td></tr></table></figure></p>
<p>这里的隐式类LoggerToLogAdapter可以将Logger适配为Log。与Java实现Adapter模式不同的是，我们不需要去创建LoggerToLogAdapter的实例。如上代码中，创建的是Logger实例。Logger自身与Log无关，但在创建该对象的上下文中，由于我们定义了隐式类，当Scala编译器遇到该隐式类时，就会为Logger添加通过隐式类定义的代码，包括隐式类中定义的对Log的继承，以及额外增加的warning与error方法。</p>
<p>在大多数场景，Adapter关注的是接口之间的适配。但是，当要适配的接口只有一个函数时，在支持高阶函数（甚至只要支持Lambda）的语言中，此时的Adapter模式就味如鸡肋了。假设Log与Logger接口只有一个log函数（不管它的函数名是什么），接收的参数为(Level, String)，那么从抽象的角度来看，它们其实属于相同的一个抽象：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f: (<span class="type">Level</span>, <span class="type">String</span>) =&gt; <span class="type">Unit</span></span><br></pre></td></tr></table></figure></p>
<p>任何一个符合该定义的函数，都是完全适配的，没有类型与函数名的约束。</p>
<p>如果再加上泛型，抽象会更加彻底。例如典型的Load Pattern实现：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">using</span></span>[<span class="type">A</span>](r : <span class="type">Resource</span>)(f : <span class="type">Resource</span> =&gt; <span class="type">A</span>) : <span class="type">A</span> =</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        f(r)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        r.dispose()</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>泛型A可以是任何类型，包括Unit类型。这里的f扩大了抽象范围，只要满足从Resource转换到A的语义，都可以传递给using函数。更而甚者可以完全抛开对Resource类型的依赖，只需要定义了close()方法，都可以作为参数传入：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">using</span></span>[<span class="type">A</span> &lt;: <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>():<span class="type">Unit</span>, <span class="type">B</span>][resource: <span class="type">A</span>](f: <span class="type">A</span> =&gt; <span class="type">B</span>): <span class="type">B</span> =</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        f(resource)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        resource.close()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">using(io.<span class="type">Source</span>.fromFile(<span class="string">"example.txt"</span>)) &#123; source =&gt; &#123;</span><br><span class="line">    <span class="keyword">for</span> (line &lt;- source.getLines) &#123;</span><br><span class="line">        println(line)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>因为FileResource定义了close()函数，所以可以作为参数传给using()函数。</p>
<h2 id="Value-Object"><a href="#Value-Object" class="headerlink" title="Value Object"></a>Value Object</h2><p>Value Object来自DDD中的概念，通常指的是没有唯一标识的不变对象。Java没有Value Object的语法，然而因其在多数业务领域中被频繁使用，Scala为其提供了快捷语法Case Class。在几乎所有的Scala项目中，都可以看到Case Class的身影。除了在业务中表现Value Object之外，还可以用于消息传递（例如AKKA在Actor之间传递的消息）、序列化等场景。此外，Case Class又可以很好地支持模式匹配，或者作为典型的代数数据类型（ADT）。例如Scala中的List，可以被定义为：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">sealed</span> <span class="class"><span class="keyword">trait</span> <span class="title">List</span>[+<span class="type">T</span>]</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">object</span> <span class="title">Nil</span> <span class="keyword">extends</span> <span class="title">List</span>[<span class="type">Nothing</span>]</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Cons</span>[+<span class="type">T</span>](<span class="params">h: <span class="type">T</span>, t: <span class="type">List</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">List</span>[<span class="type">T</span>]</span></span><br></pre></td></tr></table></figure>
<p>这里，case object是一个单例的值对象。而Nil与Cons又都同时继承自一个sealed trait。在消息定义时，我们常常采用这样的ADT定义。例如List定义中，Nil与Cons就是List ADT的sum或者union，而Cons构造器则被称之为是参数h（代表List的head）与t（代表List的tail）的product。这也是ADT（algebraic data type）之所以得名。注意它与OO中的ADT（抽象数据类型）是风马牛不相及的两个概念。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Scala/">Scala</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Scala/">Scala</a><a href="/tags/设计模式/">设计模式</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/15/Spark的UDF/" title="Spark的UDF" itemprop="url">Spark的UDF</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-03-15T02:52:24.000Z" itemprop="datePublished"> Published 2017-03-15</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>参考:<br><a href="http://zhangyi.farbox.com/post/framework/udf-and-udaf-in-spark" target="_blank" rel="external">http://zhangyi.farbox.com/post/framework/udf-and-udaf-in-spark</a><br><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank" rel="external">https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$</a></p>
<p>UDF的引入极大地丰富了Spark SQL的表现力。一方面，它让我们享受了利用Scala（当然，也包括Java或Python）更为自然地编写代码实现函数的福利，另一方面，又能精简SQL（或者DataFrame的API），更加写意自如地完成复杂的数据分析。尤其采用SQL语句去执行数据分析时，UDF帮助我们在SQL函数与Scala函数之间左右逢源，还可以在一定程度上化解不同数据源具有歧异函数的尴尬。想想不同关系数据库处理日期或时间的函数名称吧！</p>
<h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h2><h3 id="注册制"><a href="#注册制" class="headerlink" title="注册制"></a>注册制</h3><p>用Scala编写的UDF与普通的Scala函数没有任何区别，唯一需要多执行的一个步骤是要让SQLContext注册它。例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">len</span></span>(bookTitle: <span class="type">String</span>):<span class="type">Int</span> = bookTitle.length</span><br><span class="line"></span><br><span class="line">sqlContext.udf.register(<span class="string">"len"</span>, len _)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> booksWithLongTitle = sqlContext.sql(<span class="string">"select title, author from books where len(title) &gt; 10"</span>)</span><br></pre></td></tr></table></figure>
<p>编写的UDF可以放到SQL语句的fields部分，也可以作为where、groupBy或者having子句的一部分。</p>
<p>若使用DataFrame的API，则可以以字符串的形式将UDF传入：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> booksWithLongTitle = dataFrame.filter(<span class="string">"longLength(title, 10)"</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="非注册制"><a href="#非注册制" class="headerlink" title="非注册制"></a>非注册制</h3><p>DataFrame的API也可以接收Column对象，可以用$符号来包裹一个字符串表示一个Column。$是定义在SQLContext对象implicits中的一个隐式转换。此时，UDF的定义也不相同，不能直接定义Scala函数，而是要用定义在org.apache.spark.sql.functions中的udf方法来接收一个函数。这种方式无需register：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> longLength = udf((bookTitle: <span class="type">String</span>, length: <span class="type">Int</span>) =&gt; bookTitle.length &gt; length)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"><span class="keyword">val</span> booksWithLongTitle = dataFrame.filter(longLength($<span class="string">"title"</span>, $<span class="string">"10"</span>))</span><br></pre></td></tr></table></figure></p>
<p>不幸，运行这段代码会抛出异常：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cannot resolve <span class="symbol">'1</span>0' given input columns id, title, author, price, publishedDate;</span><br></pre></td></tr></table></figure>
<p>因为采用$来包裹一个常量，会让Spark错以为这是一个Column。这时，需要定义在org.apache.spark.sql.functions中的lit函数来帮助：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> booksWithLongTitle = dataFrame.filter(longLength($<span class="string">"title"</span>, lit(<span class="number">10</span>)))</span><br></pre></td></tr></table></figure></p>
<h2 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h2><p>参考 <a href="https://databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html" target="_blank" rel="external">https://databricks.com/blog/2015/09/16/apache-spark-1-5-dataframe-api-highlights.html</a><br>普通的UDF却也存在一个缺陷，就是无法在函数内部支持对表数据的聚合运算。例如，当我要对销量执行年度同比计算，就需要对当年和上一年的销量分别求和，然后再利用同比公式进行计算。此时，UDF就无能为力了。该UDAF（User Defined Aggregate Function）粉墨登场的时候了。</p>
<p>Spark为所有的UDAF定义了一个父类UserDefinedAggregateFunction。要继承这个类，需要实现父类的几个抽象方法：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span>  输入参数类型，映射为每一个<span class="type">Field</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span>   中间结果类型</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span>   返回结果</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span>  对于一组输入是否输出相同的结果</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span>  初始化buffer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span>  更新row和buffer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span>  merge两个buffer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> 计算最终结果</span><br></pre></td></tr></table></figure></p>
<p>可以将inputSchema理解为UDAF与DataFrame列有关的输入样式。例如年同比函数需要对某个可以运算的指标与时间维度进行处理，就需要在inputSchema中定义它们。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">  <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"metric"</span>, <span class="type">DoubleType</span>) :: <span class="type">StructField</span>(<span class="string">"timeCategory"</span>, <span class="type">DateType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>代码创建了拥有两个StructField的StructType。StructField的名字并没有特别要求，完全可以认为是两个内部结构的列名占位符。至于UDAF具体要操作DataFrame的哪个列，取决于调用者，但前提是数据类型必须符合事先的设置，如这里的DoubleType与DateType类型。这两个类型被定义在org.apache.spark.sql.types中。</p>
<p>bufferSchema用于定义存储聚合运算时产生的中间数据结果的Schema，例如我们需要存储当年与上一年的销量总和，就需要定义两个StructField：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">  <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"sumOfCurrent"</span>, <span class="type">DoubleType</span>) :: <span class="type">StructField</span>(<span class="string">"sumOfPrevious"</span>, <span class="type">DoubleType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>dataType标明了UDAF函数的返回值类型，deterministic是一个布尔值，用以标记针对给定的一组输入，UDAF是否总是生成相同的结果。</p>
<p>顾名思义，initialize就是对聚合运算中间结果的初始化，在我们这个例子中，两个求和的中间值都被初始化为0d：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  buffer.update(<span class="number">0</span>, <span class="number">0.0</span>)</span><br><span class="line">  buffer.update(<span class="number">1</span>, <span class="number">0.0</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>update函数的第一个参数为bufferSchema中两个Field的索引，默认以0开始，所以第一行就是针对“sumOfCurrent”的求和值进行初始化。</p>
<p>UDAF的核心计算都发生在update函数中。在我们这个例子中，需要用户设置计算同比的时间周期。这个时间周期值属于外部输入，但却并非inputSchema的一部分，所以应该从UDAF对应类的构造函数中传入。我为时间周期定义了一个样例类，且对于同比函数，我们只要求输入当年的时间周期，上一年的时间周期可以通过对年份减1来完成：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">DateRange</span>(<span class="params">startDate: <span class="type">Timestamp</span>, endDate: <span class="type">Timestamp</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">in</span></span>(targetDate: <span class="type">Date</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    targetDate.before(endDate) &amp;&amp; targetDate.after(startDate)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YearOnYearBasis</span>(<span class="params">current: <span class="type">DateRange</span></span>) <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (current.in(input.getAs[<span class="type">Date</span>](<span class="number">1</span>))) &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = buffer.getAs[<span class="type">Double</span>](<span class="number">0</span>) + input.getAs[<span class="type">Double</span>](<span class="number">0</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> previous = <span class="type">DateRange</span>(subtractOneYear(current.startDate), subtractOneYear(current.endDate))</span><br><span class="line">    <span class="keyword">if</span> (previous.in(input.getAs[<span class="type">Date</span>](<span class="number">1</span>))) &#123;</span><br><span class="line">      buffer(<span class="number">1</span>) = buffer.getAs[<span class="type">Double</span>](<span class="number">0</span>) + input.getAs[<span class="type">Double</span>](<span class="number">0</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>update函数的第二个参数input: Row对应的并非DataFrame的行，而是被inputSchema投影了的行。以本例而言，每一个input就应该只有两个Field的值。倘若我们在调用这个UDAF函数时，分别传入了销量和销售日期两个列的话，则input(0)代表的就是销量，input(1)代表的就是销售日期。</p>
<p>merge函数负责合并两个聚合运算的buffer，再将其存储到MutableAggregationBuffer中：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  buffer1(<span class="number">0</span>) = buffer1.getAs[<span class="type">Double</span>](<span class="number">0</span>) + buffer2.getAs[<span class="type">Double</span>](<span class="number">0</span>)</span><br><span class="line">  buffer1(<span class="number">1</span>) = buffer1.getAs[<span class="type">Double</span>](<span class="number">1</span>) + buffer2.getAs[<span class="type">Double</span>](<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>最后，由evaluate函数完成对聚合Buffer值的运算，得到最后的结果：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (buffer.getDouble(<span class="number">1</span>) == <span class="number">0.0</span>)</span><br><span class="line">    <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    (buffer.getDouble(<span class="number">0</span>) - buffer.getDouble(<span class="number">1</span>)) / buffer.getDouble(<span class="number">1</span>) * <span class="number">100</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>假设我们创建了这样一个简单的DataFrame：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"TestUDF"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> <span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sqlContext.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sales = <span class="type">Seq</span>(</span><br><span class="line">  (<span class="number">1</span>, <span class="string">"Widget Co"</span>, <span class="number">1000.00</span>, <span class="number">0.00</span>, <span class="string">"AZ"</span>, <span class="string">"2014-01-01"</span>),</span><br><span class="line">  (<span class="number">2</span>, <span class="string">"Acme Widgets"</span>, <span class="number">2000.00</span>, <span class="number">500.00</span>, <span class="string">"CA"</span>, <span class="string">"2014-02-01"</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="string">"Widgetry"</span>, <span class="number">1000.00</span>, <span class="number">200.00</span>, <span class="string">"CA"</span>, <span class="string">"2015-01-11"</span>),</span><br><span class="line">  (<span class="number">4</span>, <span class="string">"Widgets R Us"</span>, <span class="number">2000.00</span>, <span class="number">0.0</span>, <span class="string">"CA"</span>, <span class="string">"2015-02-19"</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="string">"Ye Olde Widgete"</span>, <span class="number">3000.00</span>, <span class="number">0.0</span>, <span class="string">"MA"</span>, <span class="string">"2015-02-28"</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> salesRows = sc.parallelize(sales, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> salesDF = salesRows.toDF(<span class="string">"id"</span>, <span class="string">"name"</span>, <span class="string">"sales"</span>, <span class="string">"discount"</span>, <span class="string">"state"</span>, <span class="string">"saleDate"</span>)</span><br><span class="line">salesDF.registerTempTable(<span class="string">"sales"</span>)</span><br></pre></td></tr></table></figure></p>
<p>那么，要使用之前定义的UDAF，则需要实例化该UDAF类，然后再通过udf进行注册：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> current = <span class="type">DateRange</span>(<span class="type">Timestamp</span>.valueOf(<span class="string">"2015-01-01 00:00:00"</span>), <span class="type">Timestamp</span>.valueOf(<span class="string">"2015-12-31 00:00:00"</span>))</span><br><span class="line"><span class="keyword">val</span> yearOnYear = <span class="keyword">new</span> <span class="type">YearOnYearBasis</span>(current)</span><br><span class="line"></span><br><span class="line">sqlContext.udf.register(<span class="string">"yearOnYear"</span>, yearOnYear)</span><br><span class="line"><span class="keyword">val</span> dataFrame = sqlContext.sql(<span class="string">"select yearOnYear(sales, saleDate) as yearOnYear from sales"</span>)</span><br><span class="line">dataFrame.show()</span><br></pre></td></tr></table></figure></p>
<p>在使用上，除了需要对UDAF进行实例化之外，与普通的UDF使用没有任何区别。但显然，UDAF更加地强大和灵活。如果Spark自身没有提供符合你需求的函数，且需要进行较为复杂的聚合运算，UDAF是一个不错的选择。</p>
<p>通过Spark提供的UDF与UDAF，你可以慢慢实现属于自己行业的函数库，让Spark SQL变得越来越强大，对于使用者而言，却能变得越来越简单。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Spark/">Spark</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/UDF/">UDF</a><a href="/tags/Spark/">Spark</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/03/02/如何优雅地终止正在运行的Spark-Streaming程序/" title="如何优雅地终止正在运行的Spark Streaming程序" itemprop="url">如何优雅地终止正在运行的Spark Streaming程序</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-03-02T06:14:30.000Z" itemprop="datePublished"> Published 2017-03-02</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>转自: <a href="https://www.iteblog.com/archives/1890.html" target="_blank" rel="external">https://www.iteblog.com/archives/1890.html</a></p>
<p>一直运行的Spark Streaming程序如何关闭呢？是直接使用kill命令强制关闭吗？这种手段是可以达到关闭的目的，但是带来的后果就是可能会导致数据的丢失，因为这时候如果程序正在处理接收到的数据，但是由于接收到kill命令，那它只能停止整个程序，而那些正在处理或者还没有处理的数据可能就会被丢失。那我们咋办？这里有两种方法。</p>
<h2 id="等作业运行完再关闭"><a href="#等作业运行完再关闭" class="headerlink" title="等作业运行完再关闭"></a>等作业运行完再关闭</h2><p>我们都知道，Spark Streaming每隔batchDuration的时间会把源源不断的流数据分割成一批有限数据集，然后计算这些数据，我们可以从Spark提供的监控页面看到当前batch是否执行完成，当作业执行完，我们就可以手动执行kill命令来强制关闭这个Streaming作业。这种方式的缺点就是得盯着监控页面，然后决定关不关闭，很不灵活。</p>
<h2 id="通过Spark内置机制关闭"><a href="#通过Spark内置机制关闭" class="headerlink" title="通过Spark内置机制关闭"></a>通过Spark内置机制关闭</h2><p>其实Spark内置就为我们提供了一种优雅的方法来关闭长期运行的Streaming作业，我们来看看 StreamingContext类中定义的一个 stop 方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>(stopSparkContext: <span class="type">Boolean</span>, stopGracefully: <span class="type">Boolean</span>)</span><br></pre></td></tr></table></figure>
<p>官方文档对其解释是：Stop the execution of the streams, with option of ensuring all received data has been processed. 控制所有接收的数据是否被处理的参数就是 stopGracefully，如果我们将它设置为true，Spark则会等待所有接收的数据被处理完成，然后再关闭计算引擎，这样就可以避免数据的丢失。现在的问题是我们在哪里调用这个stop方法？</p>
<p>Spark 1.4版本之前<br>在Spark 1.4版本之前，我们需要手动调用这个 stop 方法，一种比较合适的方式是通过 Runtime.getRuntime().addShutdownHook 来添加一个钩子，其会在JVM关闭的之前执行传递给他的函数，如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Runtime</span>.getRuntime().addShutdownHook(<span class="keyword">new</span> <span class="type">Thread</span>() &#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">    log(<span class="string">"Gracefully stop Spark Streaming"</span>)</span><br><span class="line">    streamingContext.stop(<span class="literal">true</span>, <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure></p>
<p>如果你使用的是Scala，我们还可以通过以下的方法实现类似的功能：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala.sys.addShutdownHook(&#123;</span><br><span class="line">  streamingContext.stop(<span class="literal">true</span>,<span class="literal">true</span>)</span><br><span class="line">)&#125;)</span><br></pre></td></tr></table></figure></p>
<p>通过上面的办法，我们客户确保程序退出之前会执行上面的函数，从而保证Streaming程序关闭的时候不丢失数据。</p>
<p>Spark 1.4版本之后<br>上面方式可以达到我们的需求，但是在每个程序里面都添加这样的重复代码也未免太过麻烦了！值得高兴的是，从Apache Spark 1.4版本开始，Spark内置提供了spark.streaming.stopGracefullyOnShutdown参数来决定是否需要以Gracefully方式来关闭Streaming程序（详情请参见SPARK-7776）。Spark会在启动 StreamingContext 的时候注册这个钩子，如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">shutdownHookRef = <span class="type">ShutdownHookManager</span>.addShutdownHook(</span><br><span class="line">          <span class="type">StreamingContext</span>.<span class="type">SHUTDOWN_HOOK_PRIORITY</span>)(stopOnShutdown)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">stopOnShutdown</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> stopGracefully = conf.getBoolean(<span class="string">"spark.streaming.stopGracefullyOnShutdown"</span>, <span class="literal">false</span>)</span><br><span class="line">    logInfo(<span class="string">s"Invoking stop(stopGracefully=<span class="subst">$stopGracefully</span>) from shutdown hook"</span>)</span><br><span class="line">    <span class="comment">// Do not stop SparkContext, let its own shutdown hook stop it</span></span><br><span class="line">    stop(stopSparkContext = <span class="literal">false</span>, stopGracefully = stopGracefully)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>从上面的代码可以看出，我们可以根据自己的需求来设置 spark.streaming.stopGracefullyOnShutdown 的值，而不需要在每个Streaming程序里面手动调用StreamingContext的stop方法，确实方便多了。不过虽然这个参数在Spark 1.4开始引入，但是却是在Spark 1.6才开始才有文档正式介绍（可以参见<a href="https://github.com/apache/spark/pull/8898和http://spark.apache.org/docs/1.6.0/configuration.html）" target="_blank" rel="external">https://github.com/apache/spark/pull/8898和http://spark.apache.org/docs/1.6.0/configuration.html）</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Spark/">Spark</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Spark-Streaming/">Spark Streaming</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/02/23/Spark有用的配置选项/" title="Spark有用的配置选项" itemprop="url">Spark有用的配置选项</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-02-23T08:58:04.000Z" itemprop="datePublished"> Published 2017-02-23</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="任务提交"><a href="#任务提交" class="headerlink" title="任务提交"></a>任务提交</h2><ul>
<li><figure class="highlight plain"><figcaption><span>spark任务对应的Jar，配置该选项后，每次提交任务不用上传该assembly.jar,减少了任务启动时间</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">``` shell</span><br><span class="line">spark.yarn.jar hdfs://****:****/path/spark/spark-assembly-***.jar</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight plain"><figcaption><span>yarn提交任务时的黑名单</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">## Shuffle</span><br><span class="line"></span><br><span class="line">- ```spark.shuffle.file.buffer``` </span><br><span class="line"></span><br><span class="line">Size of the in-memory buffer for each shuffle file output stream. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files.</span><br><span class="line"></span><br><span class="line">## Memory</span><br><span class="line"></span><br><span class="line">- ```spark.yarn.executor.memoryOverhead``` </span><br><span class="line">The amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Spark SQL</span><br><span class="line"></span><br><span class="line">- ``` spark.sql.shuffle.partitions</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>Spark SQL 2.0版本之前，reduce的个数是通过spark.default.parallelism和spark.sql.shuffle.partitions两个参数进行配置。如果配置过大，将会导致下游产生很多碎片化的Task，或者下游HDFS产生很多小文件。如果设置过小，将会导致单个ReduceTask计算负载过大。</p>
<ul>
<li><figure class="highlight plain"><figcaption><span>```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Spark SQL 2.0+支持通过spark.sql.adaptive.enabled来设置reduce大小自适应</span><br><span class="line">- ```spark.sql.files.ignoreCorruptFiles</span><br></pre></td></tr></table></figure></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">


</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/01/12/Spark2-0-DataSource-API/" title="Spark2.0 DataSource API" itemprop="url">Spark2.0 DataSource API</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-01-12T07:51:50.000Z" itemprop="datePublished"> Published 2017-01-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="Spark-1-如何实现Spark-External-Datasource"><a href="#Spark-1-如何实现Spark-External-Datasource" class="headerlink" title="Spark 1 如何实现Spark External Datasource"></a>Spark 1 如何实现Spark External Datasource</h2><p><a href="https://www.mapr.com/blog/spark-data-source-api-extending-our-spark-sql-query-engine" target="_blank" rel="external">Spark Data Source API: Extending Our Spark SQL Query Engine</a></p>
<p><a href="http://blog.csdn.net/oopsoom/article/details/42061077" target="_blank" rel="external">Spark SQL之External DataSource外部数据源（一）示例</a></p>
<p><a href="http://blog.csdn.net/oopsoom/article/details/42064075" target="_blank" rel="external">Spark SQL之External DataSource外部数据源（二）源码分析</a></p>
<p>参考实现：</p>
<ul>
<li><a href="https://github.com/SequoiaDB/spark-sequoiadb" target="_blank" rel="external">spark-sequoiadb</a></li>
<li><a href="https://github.com/databricks/spark-csv" target="_blank" rel="external">spark-csv</a></li>
<li><a href="https://github.com/RedisLabs/spark-redis" target="_blank" rel="external">spark-redis</a></li>
<li><a href="http://geek.csdn.net/news/detail/76853" target="_blank" rel="external">Spark多数据源计算实践及其在GrowingIO的实践</a></li>
</ul>
<p><a href="http://wiki.baidu.com/pages/viewpage.action?pageId=213816907" target="_blank" rel="external">no</a></p>
<h2 id="Spark-2-如何实现Spark-External-Datasource"><a href="#Spark-2-如何实现Spark-External-Datasource" class="headerlink" title="Spark 2 如何实现Spark External Datasource"></a>Spark 2 如何实现Spark External Datasource</h2><p><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource-api.html" target="_blank" rel="external">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource-api.html</a></p>
<p><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource.html" target="_blank" rel="external">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource.html</a></p>
<p><a href="http://www.spark.tc/exploring-the-apache-spark-datasource-api/" target="_blank" rel="external">http://www.spark.tc/exploring-the-apache-spark-datasource-api/</a></p>
<p>新特性：</p>
<ul>
<li>子查询的自持</li>
<li>更加丰富的读写api支持，包括<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">- RelationProvider</span><br><span class="line"></span><br><span class="line">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource-api.html</span><br><span class="line"></span><br><span class="line">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-sql-datasource.html</span><br><span class="line"></span><br><span class="line">http://www.spark.tc/exploring-the-apache-spark-datasource-api/</span><br><span class="line"></span><br><span class="line">[关键](http://wiki.baidu.com/pages/viewpage.action?pageId=213816907)</span><br><span class="line"></span><br><span class="line">- DataSourceRegister</span><br><span class="line"></span><br><span class="line">triat ```DataSourceRegister``` is an interface to register DataSources under their shortName aliases (to look them up later).</span><br><span class="line">``` scala</span><br><span class="line">package org.apache.spark.sql.sources</span><br><span class="line"></span><br><span class="line">trait DataSourceRegister &#123;</span><br><span class="line">  def shortName(): String</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>It allows users to use the data source alias as the format type over the fully qualified class name.</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Spark/">Spark</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Spark2-0/">Spark2.0</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/01/12/Hive-MapReduce-Spark分布式生成唯一数值型ID/" title="Hive MapReduce Spark分布式生成唯一数值型ID" itemprop="url">Hive MapReduce Spark分布式生成唯一数值型ID</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-01-12T06:25:35.000Z" itemprop="datePublished"> Published 2017-01-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>转载自： <a href="http://lxw1234.com/archives/2016/12/798.htm" target="_blank" rel="external">http://lxw1234.com/archives/2016/12/798.htm</a></p>
<p>在实际业务场景下，经常会遇到在Hive、MapReduce、Spark中需要生成唯一的数值型ID。</p>
<p>一般常用的做法有：</p>
<ol>
<li>MapReduce中使用1个Reduce来生成；</li>
<li>Hive中使用row_number分析函数来生成，其实也是1个Reduce；</li>
<li>借助HBase或Redis或Zookeeper等其它框架的计数器来生成；</li>
</ol>
<p>数据量不大的情况下，可以直接使用1和2方法来生成，但如果数据量巨大，1个Reduce处理起来就非常慢。</p>
<p>在数据量非常大的情况下，如果你仅仅需要唯一的数值型ID，注意：不是需要”连续的唯一的数值型ID”，那么可以考虑采用本文中介绍的方法，否则，请使用第3种方法来完成。</p>
<p>Spark中生成这样的非连续唯一数值型ID，非常简单，直接使用zipWithUniqueId()即可。</p>
<p>关于zipWithUniqueId，可参考：<a href="http://lxw1234.com/archives/2015/07/352.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/07/352.htm</a></p>
<p>参考zipWithUniqueId()的方法，在MapReduce和Hive中，实现如下：<br><img src="http://7xipth.com1.z0.glb.clouddn.com/20161206-5.jpg" alt=""></p>
<p>在Spark中，zipWithUniqueId是通过使用分区Index作为每个分区ID的开始值，在每个分区内，ID增长的步长为该RDD的分区数，那么在MapReduce和Hive中，也可以照此思路实现，Spark中的分区数，即为MapReduce中的Map数，Spark分区的Index，即为Map Task的ID。Map数，可以通过JobConf的getNumMapTasks()，而Map Task ID，可以通过参数mapred.task.id获取，格式如：attempt_1478926768563_0537_m_000004_0，截取m_000004_0中的4，再加1，作为该Map Task的ID起始值。注意：这两个只均需要在Job运行时才能获取。另外，从图中也可以看出，每个分区/Map Task中的数据量不是绝对一致的，因此，生成的ID不是连续的。</p>
<p>下面的UDF可以在Hive中直接使用：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lxw1234.hive.udf;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.MapredContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.UDFType;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.udf.generic.GenericUDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"> </span><br><span class="line"><span class="meta">@UDFType</span>(deterministic = <span class="keyword">false</span>, stateful = <span class="keyword">true</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RowSeq2</span> <span class="keyword">extends</span> <span class="title">GenericUDF</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> LongWritable result = <span class="keyword">new</span> LongWritable();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">char</span> SEPARATOR = <span class="string">'_'</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String ATTEMPT = <span class="string">"attempt"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">long</span> initID = <span class="number">0l</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> increment = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(MapredContext context)</span> </span>&#123;</span><br><span class="line">        increment = context.getJobConf().getNumMapTasks();</span><br><span class="line">        <span class="keyword">if</span>(increment == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"mapred.map.tasks is zero"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        initID = getInitId(context.getJobConf().get(<span class="string">"mapred.task.id"</span>),increment);</span><br><span class="line">        <span class="keyword">if</span>(initID == <span class="number">0l</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"mapred.task.id"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        System.out.println(<span class="string">"initID : "</span> + initID + <span class="string">"  increment : "</span> + increment);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ObjectInspector <span class="title">initialize</span><span class="params">(ObjectInspector[] arguments)</span></span><br><span class="line">            <span class="keyword">throws</span> UDFArgumentException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> PrimitiveObjectInspectorFactory.writableLongObjectInspector;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">evaluate</span><span class="params">(DeferredObject[] arguments)</span> <span class="keyword">throws</span> HiveException </span>&#123;</span><br><span class="line">        result.set(getValue());</span><br><span class="line">        increment(increment);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getDisplayString</span><span class="params">(String[] children)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"RowSeq-func()"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">increment</span><span class="params">(<span class="keyword">int</span> incr)</span> </span>&#123;</span><br><span class="line">        initID += incr;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">getValue</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> initID;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//attempt_1478926768563_0537_m_000004_0 // return 0+1</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">getInitId</span> <span class="params">(String taskAttemptIDstr,<span class="keyword">int</span> numTasks)</span></span><br><span class="line">            <span class="keyword">throws</span> IllegalArgumentException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            String[] parts = taskAttemptIDstr.split(Character.toString(SEPARATOR));</span><br><span class="line">            <span class="keyword">if</span>(parts.length == <span class="number">6</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span>(parts[<span class="number">0</span>].equals(ATTEMPT)) &#123;</span><br><span class="line">                    <span class="keyword">if</span>(!parts[<span class="number">3</span>].equals(<span class="string">"m"</span>) &amp;&amp; !parts[<span class="number">3</span>].equals(<span class="string">"r"</span>)) &#123;</span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> Exception();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">long</span> result = Long.parseLong(parts[<span class="number">4</span>]);</span><br><span class="line">                    <span class="keyword">if</span>(result &gt;= numTasks) &#123; <span class="comment">//if taskid &gt;= numtasks</span></span><br><span class="line">                        <span class="keyword">throw</span> <span class="keyword">new</span> Exception(<span class="string">"TaskAttemptId string : "</span> + taskAttemptIDstr</span><br><span class="line">                                + <span class="string">"  parse ID ["</span> + result + <span class="string">"] &gt;= numTasks["</span> + numTasks + <span class="string">"] .."</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">return</span> result + <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;&#125;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"TaskAttemptId string : "</span> + taskAttemptIDstr</span><br><span class="line">                + <span class="string">" is not properly formed"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Hive/">Hive</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Hive/">Hive</a><a href="/tags/UDF/">UDF</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/01/12/Spark2-0学习笔记/" title="Spark2.0学习笔记" itemprop="url">Spark2.0学习笔记</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Sun Ke" target="_blank" itemprop="author">Sun Ke</a>
		
  <p class="article-time">
    <time datetime="2017-01-12T03:18:42.000Z" itemprop="datePublished"> Published 2017-01-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="Catalog-API"><a href="#Catalog-API" class="headerlink" title="Catalog API"></a>Catalog API</h2><p>Spark中的DataSet和Dataframe API支持结构化分析。结构化分析的一个重要的方面是管<br>理元数据。这些元数据可能是一些临时元数据（比如临时表）、SQLContext上注册的UDF以及<br>持久化的元数据（比如Hivemeta store或者HCatalog）。<br>Spark的早期版本是没有标准的API来访问这些元数据的。用户通常使用查询语句（比<br>如 show tables ）来查询这些元数据。这些查询通常需要操作原始的字符串，而且不同元数据<br>类型的操作也是不一样的。<br>这种情况在Spark 2.0中得到改变。Spark 2.0中添加了标准的API（称为catalog）来访问<br>Spark SQL中的元数据。这个API既可以操作Spark SQL，也可以操作Hive元数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder.appName(<span class="string">"spark session example"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"><span class="keyword">val</span> catalog = sparkSession.catalog</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Querying the databases<br>一旦创建好catalog对象之后，可以使用它来查询元数据中的数据库，catalog上的<br>API返回的结果全部都是dataset。<br>listDatabases 返回元数据中所有的数据库。默认情况下，元数据仅仅只有名为default的数<br>据库。如果是Hive元数据，那么它会从Hive元数据中获取所有的数据库。 listDatabases 返<br>回的类型是dataset，所以我们可以使用Dataset上的所有操作来查询元数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catalog.listDatabases().select(<span class="string">"name"</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用createTempView注册Dataframe<br>在Spark的早期版本，我们使用 registerTempTable 来注册Dataframe。然而在Spark<br>2.0中，这个API已经被遗弃了。 registerTempTable 名字很让人误解，因为用户会认为这个函<br>数会将Dataframe持久化并且保证这个临时表，但是实际上并不是这样的，所以社区才有意将它<br>替换成 createTempView 。 createTempView 的使用方法如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createTempView(<span class="string">"iteblog"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询表<br>正如我们可以展示出元数据中的所有数据库一样，我们也可以展示出元数据中某个数据库中<br>的表。它会展示出Spark SQL中所有注册的临时表。同时可以展示出Hive中默认数据库（也就是<br>default）中的表。如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catalog.listTables().select(<span class="string">"name"</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>判断某个表是否缓存</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">catalog.isCached(<span class="string">"iteblog"</span>)</span><br><span class="line">df.cache()</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除view<br>以使用catalog提供的API来删除view。如果是Spark SQL情况，那么它会删除事先注册好的view；如果是hive情况，那么它会从元数据中删除表</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catalog.dropTempView(<span class="string">"iteblog"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询已经注册的函数<br>仅可以使用Catalog API操作表，还可以用它操作UDF。下面代码片段展示<br>SparkSession上所有已经注册号的函数，当然也包括了Spark内置的函数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catalog.listFunctions().select(<span class="string">"name"</span>,<span class="string">"className"</span>,<span class="string">"isTemporary"</span>).show(<span class="number">100</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Catalyst优化器"><a href="#Catalyst优化器" class="headerlink" title="Catalyst优化器"></a>Catalyst优化器</h2><p>Spark SQL使用Catalyst优化所有的查询，包括spark sql和dataframe dsl。这个优化器的使用使得查询比直接使用RDD要快很多。Spark在每个版本都会对Catalyst进行优化以便提高查询性能，而不需要用户修改他们的代码。<br>Catalyst是一个单独的模块类库，这个模块是基于规则的系统。这个框架中的每个规则都是针对某个特定的情况来优化的。比如： ConstantFolding 规则用于移除查询中的常量表达式。<br>在Spark的早期版本，如果需要添加自定义的优化规则，我们需要修改Spark的源码，这在很多情况下是不太可取的，比如我们仅仅需要优化特定的领域或者场景。所以开发社区想有一种可插拨的方式在Catalyst中添加优化规则。值得高兴的是，Spark 2.0提供了这种实验式的API，我们可以基于这些API添加自定义的优化规则。本文将介绍如何编写自定义的优化规则，并将这些规则添加到Catalyst中。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"> 	<span class="comment">// dataframe的优化计划(Optimized plan)</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.option(<span class="string">"header"</span>, <span class="string">"true"</span>).csv(<span class="string">"file:///user/iteblog/sales.csv"</span>)</span><br><span class="line">    <span class="keyword">val</span> multipliedDF = df.selectExpr(<span class="string">"amountPaid * 1"</span>)</span><br><span class="line">    println(multipliedDF.queryExecution.optimizedPlan.numberedTreeString)</span><br><span class="line">	<span class="comment">// Spark自动对每一行的 amountPaid 乘上 1.0 。但是这不是最优计划！因为如果是乘以1，最终的结果是一样的。所有我们可以利用这个知识来编写自定义的优化规则，并将这个规则加入到Catalyst中</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义优化计划</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.catalyst.expressions.&#123;<span class="type">Literal</span>, <span class="type">Multiply</span>&#125;</span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.catalyst.plans.logical.<span class="type">LogicalPlan</span></span><br><span class="line">    <span class="keyword">import</span> org.apache.spark.sql.catalyst.rules.<span class="type">Rule</span></span><br><span class="line">    <span class="class"><span class="keyword">object</span> <span class="title">MultiplyOptimizationRule</span> <span class="keyword">extends</span> <span class="title">Rule</span>[<span class="type">LogicalPlan</span>] </span>&#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">LogicalPlan</span> = plan transformAllExpressions &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Multiply</span>(left, right) <span class="keyword">if</span> right.isInstanceOf[<span class="type">Literal</span>] &amp;&amp;</span><br><span class="line">          right.asInstanceOf[<span class="type">Literal</span>].value.asInstanceOf[<span class="type">Double</span>] == <span class="number">1.0</span> =&gt;</span><br><span class="line">          println(<span class="string">"optimization of one applied"</span>)</span><br><span class="line">          left</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// 我们扩展了Rule，Rule是直接操作逻辑计划的。绝大多数的规则都是使用Scala中的模式匹</span></span><br><span class="line">配。在上面的代码中，我们首先判断优化的操作数(operand)是否是文字(literal)，然后判断其值</span><br><span class="line">是否是<span class="number">1.0</span>。为了简便起见，我们限定了<span class="number">1</span>出现的位置，如果<span class="number">1</span>出现在左边，这个优化规则将不起</span><br><span class="line">作用。但是我们可以仿照上面的示例轻松地实现.通过上面的规则，如果右边的值是<span class="number">1</span>，我们将直接返回左边的值。</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 将自定义的优化规则加入到Catalyst中</span></span><br><span class="line">    spark.experimental.extraOptimizations = <span class="type">Seq</span>(<span class="type">MultiplyOptimizationRule</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 使用自定义优化规则</span></span><br><span class="line">    <span class="keyword">val</span> newMultipliedDF = df.selectExpr(<span class="string">"amountPaid * 1"</span>)</span><br><span class="line">    println(newMultipliedDF.queryExecution.optimizedPlan.numberedTreeString)</span><br></pre></td></tr></table></figure>
<h2 id="Spark-SQL中Window-API"><a href="#Spark-SQL中Window-API" class="headerlink" title="Spark SQL中Window API"></a>Spark SQL中Window API</h2><p>Spark SQL中的window API是从1.4版本开始引入的，以便支持更智能的分组功能。这个功<br>能对于那些有SQL背景的人来说非常有用；但是在Spark 1.x中，window API一大缺点就是无法<br>使用时间来创建窗口。时间在诸如金融、电信等领域有着非常重要的角色，基于时间来理解数据<br>变得至关重要。<br>不过值得高兴的是，在Spark 2.0中，window API内置也支持time windows！Spark SQL<br>中的time windows和Spark Streaming中的time windows非常类似。</p>
<ul>
<li><p>tumbling window<br>所有的timewindow API需要一个类型为timestamp的列。<figure class="highlight plain"><figcaption><span>by```语句中使用。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">window方法的第一个参数指定了时间所在的列；第二个参数指定了窗口的持续时间</span><br><span class="line">(duration)，它的单位可以是seconds、minutes、hours、days或者weeks。创建好窗口之</span><br><span class="line">后，我们可以计算平均值。</span><br><span class="line">``` scala</span><br><span class="line">// 使用了内置的year函数来提取出日期中的年</span><br><span class="line">val stocks2016 = stocksDF.filter(&quot;year(Date)==2016&quot;)</span><br><span class="line"></span><br><span class="line">// 对每个星期创建一个窗口，这种类型的窗口通常被称为tumbling window</span><br><span class="line">val tumblingWindowDS = stocks2016.groupBy(window(stocks2016.col(&quot;Date&quot;),&quot;1 week&quot;)).agg(avg(&quot;Close&quot;).as(&quot;weekly_average&quot;))</span><br><span class="line"></span><br><span class="line">def printWindow(windowDF:DataFrame, aggCol:String) =&#123;</span><br><span class="line">windowDF.sort(&quot;window.start&quot;).</span><br><span class="line">select(&quot;window.start&quot;,&quot;window.end&quot;,s&quot;$aggCol&quot;).</span><br><span class="line">show(truncate = false)</span><br><span class="line">&#125;</span><br><span class="line">// 打印window的值</span><br><span class="line">printWindow(tumblingWindowDS,&quot;weekly_average&quot;)</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>用sliding window（滑动窗口）<br>到目前为止，没有相关API来创建带有开始时间的tumbling<br>window，但是我们可以通过将窗口时间(window duration)和滑动时间(slide duration)设置成<br>一样来创建带有开始时间的tumbling window。</p>
</li>
</ul>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 4 days 参数就是开始时间的偏移量；前两个参数分别代表窗口时间和滑动时间</span><br><span class="line">val iteblogWindowWithStartTime = stocks2016.groupBy(window(stocks2016.col("Date"),"1</span><br><span class="line">week","1 week", "4 days")).agg(avg("Close").as("weekly_average"))</span><br></pre></td></tr></table></figure>
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    <footer class="article-footer clearfix">
<div class="article-catetags">

<div class="article-categories">
  <span></span>
  <a class="article-category-link" href="/categories/Spark/">Spark</a>
</div>


  <div class="article-tags">
  
  <span></span> <a href="/tags/Spark2-0/">Spark2.0</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>







  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/page/4/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/6/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  
<div class="github-card">
<p class="asidetitle">Github Card</p>
<div class="github-card" data-github="slamke" data-width="220" data-height="119" data-theme="medium">
<script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js" ></script>
</div>
  </div>



  
<div class="categorieslist">
	<p class="asidetitle">Categories</p>
		<ul>
		
		  
			<li><a href="/categories/Akka/" title="Akka">Akka<sup>7</sup></a></li>
		  
		
		  
			<li><a href="/categories/HDFS/" title="HDFS">HDFS<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Hadoop/" title="Hadoop">Hadoop<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/Hive/" title="Hive">Hive<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/Java/" title="Java">Java<sup>12</sup></a></li>
		  
		
		  
			<li><a href="/categories/Kafka/" title="Kafka">Kafka<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/ML/" title="ML">ML<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/Python/" title="Python">Python<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/REST/" title="REST">REST<sup>2</sup></a></li>
		  
		
		  
			<li><a href="/categories/Reactor/" title="Reactor">Reactor<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Scala/" title="Scala">Scala<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/Spark/" title="Spark">Spark<sup>29</sup></a></li>
		  
		
		  
			<li><a href="/categories/Spring/" title="Spring">Spring<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/Tomcat/" title="Tomcat">Tomcat<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/Web/" title="Web">Web<sup>6</sup></a></li>
		  
		
		  
			<li><a href="/categories/多线程/" title="多线程">多线程<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/数据仓库/" title="数据仓库">数据仓库<sup>1</sup></a></li>
		  
		
		  
			<li><a href="/categories/算法/" title="算法">算法<sup>5</sup></a></li>
		  
		
		  
			<li><a href="/categories/重构/" title="重构">重构<sup>1</sup></a></li>
		  
		
		</ul>
</div>


  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/Spark/" title="Spark">Spark<sup>26</sup></a></li>
			
		
			
				<li><a href="/tags/Java/" title="Java">Java<sup>11</sup></a></li>
			
		
			
				<li><a href="/tags/Web/" title="Web">Web<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/Akka/" title="Akka">Akka<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/Spring/" title="Spring">Spring<sup>7</sup></a></li>
			
		
			
				<li><a href="/tags/Hadoop/" title="Hadoop">Hadoop<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/Hive/" title="Hive">Hive<sup>6</sup></a></li>
			
		
			
				<li><a href="/tags/Scala/" title="Scala">Scala<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/Yarn/" title="Yarn">Yarn<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/算法/" title="算法">算法<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Session/" title="Session">Session<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Cookie/" title="Cookie">Cookie<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/多线程/" title="多线程">多线程<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Spark-Streaming/" title="Spark Streaming">Spark Streaming<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/Junit/" title="Junit">Junit<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/UDF/" title="UDF">UDF<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/REST/" title="REST">REST<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/Spark2-0/" title="Spark2.0">Spark2.0<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/java/" title="java">java<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/HDFS/" title="HDFS">HDFS<sup>2</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">Links</p>
    <ul>
        
          <li>
            
            	<a href="http://slamke.blogspot.com/" target="_blank" title="我的博客">我的博客</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">Weibo</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=null&verifier=&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Larry Page in Google. <br/>
			This is my blog,believe it or not.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		<a href="https://github.com/slamke" target="_blank" class="icon-github" title="github"></a>
		
		
		
		
		
		
		
		
		
		<a href="mailto:sunke3296@gmail.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2018 
		
		<a href="/about" target="_blank" title="Sun Ke">Sun Ke</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="Back to Top"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
