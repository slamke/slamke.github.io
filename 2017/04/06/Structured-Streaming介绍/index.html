<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Spark,Streaming," />










<meta name="description" content="中文入门编程指南官方文档Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1背景Structured Streaming接口在社区2.0版本发布测试接口，主要暴露最初的设计思路及基本接口，不具备在生产环境使用的能力；2.1版本中Structured Streaming作为主要功能发布，支持Kafka数据源、基于e">
<meta name="keywords" content="Spark,Streaming">
<meta property="og:type" content="article">
<meta property="og:title" content="Structured Streaming介绍">
<meta property="og:url" content="http://slamke.github.io/2017/04/06/Structured-Streaming介绍/index.html">
<meta property="og:site_name" content="雁渡寒潭 风吹疏竹">
<meta property="og:description" content="中文入门编程指南官方文档Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1背景Structured Streaming接口在社区2.0版本发布测试接口，主要暴露最初的设计思路及基本接口，不具备在生产环境使用的能力；2.1版本中Structured Streaming作为主要功能发布，支持Kafka数据源、基于e">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png">
<meta property="og:image" content="http://spark.apache.org/docs/latest/img/structured-streaming-watermark.png">
<meta property="og:updated_time" content="2017-04-07T09:53:48.072Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Structured Streaming介绍">
<meta name="twitter:description" content="中文入门编程指南官方文档Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1背景Structured Streaming接口在社区2.0版本发布测试接口，主要暴露最初的设计思路及基本接口，不具备在生产环境使用的能力；2.1版本中Structured Streaming作为主要功能发布，支持Kafka数据源、基于e">
<meta name="twitter:image" content="http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://slamke.github.io/2017/04/06/Structured-Streaming介绍/"/>





  <title>Structured Streaming介绍 | 雁渡寒潭 风吹疏竹</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">雁渡寒潭 风吹疏竹</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">教练，我想打篮球</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://slamke.github.io/2017/04/06/Structured-Streaming介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sun Ke">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雁渡寒潭 风吹疏竹">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Structured Streaming介绍</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-06T13:42:35+08:00">
                2017-04-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/04/06/Structured-Streaming介绍/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/04/06/Structured-Streaming介绍/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="中文入门编程指南"><a href="#中文入门编程指南" class="headerlink" title="中文入门编程指南"></a><a href="https://www.iteblog.com/archives/2084.html" target="_blank" rel="noopener">中文入门编程指南</a></h2><h2 id="官方文档"><a href="#官方文档" class="headerlink" title="官方文档"></a><a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener">官方文档</a></h2><h2 id="Real-time-Streaming-ETL-with-Structured-Streaming-in-Apache-Spark-2-1"><a href="#Real-time-Streaming-ETL-with-Structured-Streaming-in-Apache-Spark-2-1" class="headerlink" title="Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1"></a><a href="https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html" target="_blank" rel="noopener">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</a></h2><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Structured Streaming接口在社区2.0版本发布测试接口，主要暴露最初的设计思路及基本接口，不具备在生产环境使用的能力；2.1版本中Structured Streaming作为主要功能发布，支持Kafka数据源、基于event_time的window及watermark功能，虽然还在Alapha阶段，但从实现的完备程度及反馈来看已具备初步的功能需求。</p>
<h2 id="设计理念及所解决的问题"><a href="#设计理念及所解决的问题" class="headerlink" title="设计理念及所解决的问题"></a>设计理念及所解决的问题</h2><p>从Spark 0.7版本发布DStream接口及Spark Streaming模块以来，Spark具备流式处理功能且在业界有了一系列应用，但依旧存在一些问题，诟病较多的是如下几点：</p>
<ul>
<li>不支持event_time，按照到达绝对时间切分records组成DStream的方式对很多场景不适合</li>
<li>不支持流式window操作</li>
<li>不支持watermark，无法对乱序数据做容错</li>
</ul>
<p>2.1版本的Spark不但解决上述问题，并将Spark Streaming的流处理方式和1.x版本中集中开发的SQL模块、DataFrame\DataSet API相融合，推出Structured Streaming引擎。其设计思路在于将持续不断的上游数据抽象为unbounded table，对流式的处理看作是表中不同部分的数据(complete\append\update mode)进行处理：<br><img src="http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png" alt=""><br>从代码层面看，Structured Streaming代码放在sql模块中，与原有SQL的datasource api、logical plan、physical plan做了诸多兼容操作，将流式处理的上游下游分别抽象为Source与Sink，基于DataFrame抽象出输入输出流DataStreamReader、DataStreamWriter。DataStreamReader中打通datasource api，支持多种上游的读取支持，DataStreamWriter中遵循惰性计算的思路实现多种触发操作及不同类型的写出模式。</p>
<h2 id="Demo-for-struct-streaming"><a href="#Demo-for-struct-streaming" class="headerlink" title="Demo for struct streaming"></a>Demo for struct streaming</h2><p>一个完整的struct streaming示例及分布解释如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">// 与其他spark作业一致，获取build spark session,对应旧版本中的spark context</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder</span><br><span class="line">  .appName(<span class="string">"StructuredNetworkWordCount"</span>)</span><br><span class="line">  .getOrCreate()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">// 创建一个基于stream source的dataframe</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> lines = spark.readStream</span><br><span class="line">  .format(<span class="string">"socket"</span>)    # 对应基本概念：<span class="type">Source</span></span><br><span class="line">  .option(<span class="string">"host"</span>, host)</span><br><span class="line">  .option(<span class="string">"port"</span>, port)</span><br><span class="line">  .load()</span><br><span class="line"> </span><br><span class="line"><span class="comment">// WordCount逻辑，与dataframe/dataset api用法完全一致，</span></span><br><span class="line"><span class="keyword">val</span> words = lines.as[<span class="type">String</span>].flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.groupBy(<span class="string">"value"</span>).count()</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 调用dataset的writeStream创建写出流，设置对应的sink mode及类型</span></span><br><span class="line"><span class="keyword">val</span> query = wordCounts.writeStream</span><br><span class="line">  .outputMode(<span class="string">"complete"</span>)   # 对应基本概念：<span class="type">Output</span> <span class="type">Mode</span></span><br><span class="line">  .format(<span class="string">"console"</span>)        # 对应基本概念：<span class="type">Sink</span></span><br><span class="line">  .start()</span><br><span class="line"> </span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure></p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p>Struct Streaming的输入源，每种输入源对应自己的dataSource实现，当前支持如下三种Source：</p>
<ul>
<li><strong>File Source</strong>: 输入数据源为hdfs目录中的文件，天然支持的文件格式和dataset一致(json csv text parquet)，将文件不断mv至指定目录中作为持续数据源</li>
<li><strong>Kafka Source</strong>: 将kafka作为struct streaming source，目前支持的版本为0.10.0</li>
<li><strong>Socket Source</strong>: 将socket输入数据作为streaming source，只用来做调试和demo使用<h3 id="Output-Mode"><a href="#Output-Mode" class="headerlink" title="Output Mode"></a>Output Mode</h3>Output Mode都是对于每次trigger过后的result table而言的</li>
<li><strong>Complete Mode</strong> : 从开始到目前为止的所有数据视为一张大表，query作用于整个表上的结果整体写入</li>
<li><strong>Append Mode</strong> : 从上次trigger到目前为止，不会在发生变化的数据append到最终的sink端</li>
<li><strong>Update Mode</strong> : 从上次trigger到目前为止，发生变化的条目写入到最终的sink端</li>
</ul>
<p>简单用自带的StructuredNetworkWordCountWindowed实例对比下Complete mode与Update Mode:<br>数据输入：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[liyuanjian@MacBook~Pro ~]$ nc -l 9999</span><br><span class="line">apache spark</span><br><span class="line">apache hadoop</span><br><span class="line">baidu inf spark</span><br></pre></td></tr></table></figure></p>
<p>Complete mode output:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">+------+-----+</span><br><span class="line">| value|count|</span><br><span class="line">+------+-----+</span><br><span class="line">|apache| 1|</span><br><span class="line">| spark| 1|</span><br><span class="line">+------+-----+</span><br><span class="line"> </span><br><span class="line">+------+-----+</span><br><span class="line">| value|count|</span><br><span class="line">+------+-----+</span><br><span class="line">|apache| 2|</span><br><span class="line">|hadoop| 1|</span><br><span class="line">| spark| 1|</span><br><span class="line">+------+-----+</span><br><span class="line"> </span><br><span class="line">+------+-----+</span><br><span class="line">| value|count|</span><br><span class="line">+------+-----+</span><br><span class="line">| baidu| 1|</span><br><span class="line">|apache| 2|</span><br><span class="line">|hadoop| 1|</span><br><span class="line">| spark| 2|</span><br><span class="line">| inf| 1|</span><br><span class="line">+------+-----+</span><br></pre></td></tr></table></figure></p>
<p>Update mode output:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">+------+-----+</span><br><span class="line">| value|count|</span><br><span class="line">+------+-----+</span><br><span class="line">|apache| 1|</span><br><span class="line">| spark| 1|</span><br><span class="line">+------+-----+</span><br><span class="line"> </span><br><span class="line">+------+-----+</span><br><span class="line">| value|count|</span><br><span class="line">+------+-----+</span><br><span class="line">|apache| 2|</span><br><span class="line">|hadoop| 1|</span><br><span class="line">+------+-----+</span><br><span class="line"> </span><br><span class="line">+-----+-----+</span><br><span class="line">|value|count|</span><br><span class="line">+-----+-----+</span><br><span class="line">|baidu| 1|</span><br><span class="line">| inf| 1|</span><br><span class="line">|spark| 2|</span><br><span class="line">+-----+-----+</span><br></pre></td></tr></table></figure></p>
<h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><p>作为struct streaming的下游抽象，sink代表将最终处理完成的dataframe写出的方式，目前支持：</p>
<ol>
<li><strong>File sink</strong> : 将数据写入hdfs目录，可以支持带partition的table写入</li>
<li><strong>Console sink</strong> : 将数据直接调用dataframe.show()打印在stdout，调试作用，demo中使用的都是console sink</li>
<li><strong>Memory sink</strong> : 将所有下游存储在driver的内存中，抽象为一张表，也只能做调试使用</li>
<li><strong>Foreach sink</strong> : 依赖用户实现ForeachWriter接口配合使用，foreach sink中对每次触发的dataframe，按逐个partition调用ForeachWriter的接口进行处理，可以实现ForeachWriter写入任何需要的下游存储或处理系统，接口如下：<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ForeachWriter</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Move this to org.apache.spark.sql.util or consolidate this with batch API.</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called when starting to process one partition of new data in the executor. The `version` is</span></span><br><span class="line"><span class="comment">   * for data deduplication when there are failures. When recovering from a failure, some data may</span></span><br><span class="line"><span class="comment">   * be generated multiple times but they will always have the same version.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * If this method finds using the `partitionId` and `version` that this partition has already been</span></span><br><span class="line"><span class="comment">   * processed, it can return `false` to skip the further data processing. However, `close` still</span></span><br><span class="line"><span class="comment">   * will be called for cleaning up resources.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param partitionId the partition id.</span></span><br><span class="line"><span class="comment">   * @param version a unique id for data deduplication.</span></span><br><span class="line"><span class="comment">   * @return `true` if the corresponding partition and version id should be processed. `false`</span></span><br><span class="line"><span class="comment">   *         indicates the partition should be skipped.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(partitionId: <span class="type">Long</span>, version: <span class="type">Long</span>): <span class="type">Boolean</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called to process the data in the executor side. This method will be called only when `open`</span></span><br><span class="line"><span class="comment">   * returns `true`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(value: <span class="type">T</span>): <span class="type">Unit</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called when stopping to process one partition of new data in the executor side. This is</span></span><br><span class="line"><span class="comment">   * guaranteed to be called either `open` returns `true` or `false`. However,</span></span><br><span class="line"><span class="comment">   * `close` won't be called in the following cases:</span></span><br><span class="line"><span class="comment">   *  - JVM crashes without throwing a `Throwable`</span></span><br><span class="line"><span class="comment">   *  - `open` throws a `Throwable`.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param errorOrNull the error thrown during processing data or null if there was no error.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(errorOrNull: <span class="type">Throwable</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h3><p>和其他流式系统一样，基于滑动时间窗的数据统计、聚合是必不可少的需求，2.1实现了基于event-time的window定义，接口如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Bucketize rows into one or more time windows given a timestamp specifying column. Window</span></span><br><span class="line"><span class="comment"> * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window</span></span><br><span class="line"><span class="comment"> * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in</span></span><br><span class="line"><span class="comment"> * the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC.</span></span><br><span class="line"><span class="comment"> * The following example takes the average stock price for a one minute window every 10 seconds:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment"> *   val df = ... // schema =&gt; timestamp: TimestampType, stockId: StringType, price: DoubleType</span></span><br><span class="line"><span class="comment"> *   df.groupBy(window($"time", "1 minute", "10 seconds"), $"stockId")</span></span><br><span class="line"><span class="comment"> *     .agg(mean("price"))</span></span><br><span class="line"><span class="comment"> * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The windows will look like:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &#123;&#123;&#123;</span></span><br><span class="line"><span class="comment"> *   09:00:00-09:01:00</span></span><br><span class="line"><span class="comment"> *   09:00:10-09:01:10</span></span><br><span class="line"><span class="comment"> *   09:00:20-09:01:20 ...</span></span><br><span class="line"><span class="comment"> * &#125;&#125;&#125;</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * For a streaming query, you may use the function `current_timestamp` to generate windows on</span></span><br><span class="line"><span class="comment"> * processing time.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param timeColumn The column or the expression to use as the timestamp for windowing by time.</span></span><br><span class="line"><span class="comment"> *                   The time column must be of TimestampType.</span></span><br><span class="line"><span class="comment"> * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,</span></span><br><span class="line"><span class="comment"> *                       `1 second`. Check [[org.apache.spark.unsafe.types.CalendarInterval]] for</span></span><br><span class="line"><span class="comment"> *                       valid duration identifiers. Note that the duration is a fixed length of</span></span><br><span class="line"><span class="comment"> *                       time, and does not vary over time according to a calendar. For example,</span></span><br><span class="line"><span class="comment"> *                       `1 day` always means 86,400,000 milliseconds, not a calendar day.</span></span><br><span class="line"><span class="comment"> * @param slideDuration A string specifying the sliding interval of the window, e.g. `1 minute`.</span></span><br><span class="line"><span class="comment"> *                      A new window will be generated every `slideDuration`. Must be less than</span></span><br><span class="line"><span class="comment"> *                      or equal to the `windowDuration`. Check</span></span><br><span class="line"><span class="comment"> *                      [[org.apache.spark.unsafe.types.CalendarInterval]] for valid duration</span></span><br><span class="line"><span class="comment"> *                      identifiers. This duration is likewise absolute, and does not vary</span></span><br><span class="line"><span class="comment"> *                     according to a calendar.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @group datetime_funcs</span></span><br><span class="line"><span class="comment"> * @since 2.0.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Experimental</span></span><br><span class="line"><span class="meta">@InterfaceStability</span>.<span class="type">Evolving</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">window</span></span>(timeColumn: <span class="type">Column</span>, windowDuration: <span class="type">String</span>, slideDuration: <span class="type">String</span>): <span class="type">Column</span> = &#123;</span><br><span class="line">  window(timeColumn, windowDuration, slideDuration, <span class="string">"0 second"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>window通常与groupBy算子连用，通过设置dataframe中的timeColumn，windowDuration(window大小)，sildeDuration(步长)，来定义整个window的行为，如windowDuration = 10min, slideDuration = 5min，则代表由event-time触发每5分钟计算一次，计算的对象是当前window中10min的数据</p>
<h3 id="Watermark"><a href="#Watermark" class="headerlink" title="Watermark"></a>Watermark</h3><p>接上述window的介绍，对于任何基于window\event-time的聚合场景，我们都需要考虑对于乱序的过期event-time数据到达的处理行为。watermark用来给用户提供一个接口，让用户能够定义”比当前处理时间慢多久的数据可以丢弃，不再处理”，祥设文档参见：<a href="https://docs.google.com/document/d/1z-Pazs5v4rA31azvmYhu4I5xwqaNQl6ZLIS03xhkfCQ/edit#heading=h.yx3tjr1mrnl2" target="_blank" rel="noopener">https://docs.google.com/document/d/1z-Pazs5v4rA31azvmYhu4I5xwqaNQl6ZLIS03xhkfCQ/edit#heading=h.yx3tjr1mrnl2</a></p>
<p><img src="http://spark.apache.org/docs/latest/img/structured-streaming-watermark.png" alt=""></p>
<ul>
<li>watermark的计算规则</li>
</ul>
<ol>
<li>在每次trigger触发计算时，先找到trigger data中的最大(近)event-time</li>
<li>trigger结束后，   <code>new watermark = MAX(event-time before trigger, max event-time in trigger[步骤1]) - threashold</code><br>watermark的限制<br>watermark操作只能在logical plan中有一个，而且只能应用在从sink出发的单child关系链，简单理解就是处理不了复杂的多留join、union的各自设置watermark</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
            <a href="/tags/Streaming/" rel="tag"># Streaming</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/30/如何在Spark平台搭建ThriftServer/" rel="next" title="如何在Spark平台搭建ThriftServer">
                <i class="fa fa-chevron-left"></i> 如何在Spark平台搭建ThriftServer
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/04/10/流式计算概述和Spark-Streaming-tips/" rel="prev" title="流式计算概述和Spark Streaming tips">
                流式计算概述和Spark Streaming tips <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Sun Ke</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">100</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">61</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#中文入门编程指南"><span class="nav-number">1.</span> <span class="nav-text">中文入门编程指南</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#官方文档"><span class="nav-number">2.</span> <span class="nav-text">官方文档</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Real-time-Streaming-ETL-with-Structured-Streaming-in-Apache-Spark-2-1"><span class="nav-number">3.</span> <span class="nav-text">Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#背景"><span class="nav-number">4.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#设计理念及所解决的问题"><span class="nav-number">5.</span> <span class="nav-text">设计理念及所解决的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Demo-for-struct-streaming"><span class="nav-number">6.</span> <span class="nav-text">Demo for struct streaming</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基本概念"><span class="nav-number">7.</span> <span class="nav-text">基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Source"><span class="nav-number">7.1.</span> <span class="nav-text">Source</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Output-Mode"><span class="nav-number">7.2.</span> <span class="nav-text">Output Mode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sink"><span class="nav-number">7.3.</span> <span class="nav-text">Sink</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Window"><span class="nav-number">7.4.</span> <span class="nav-text">Window</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Watermark"><span class="nav-number">7.5.</span> <span class="nav-text">Watermark</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sun Ke</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://Sunke.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://slamke.github.io/2017/04/06/Structured-Streaming介绍/';
          this.page.identifier = '2017/04/06/Structured-Streaming介绍/';
          this.page.title = 'Structured Streaming介绍';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://Sunke.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  

  

  

</body>
</html>
